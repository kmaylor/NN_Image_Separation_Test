{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D, Activation, LeakyReLU, Dropout,BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_sims=pk.load(open('Fake_sims_for_separation_test.pk','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(object):\n",
    "    \n",
    "    def __init__(self, img_rows=64, img_cols=64, channel=1):\n",
    "\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channel = channel\n",
    "        self.E = None   # endcoder\n",
    "        self.D = None   # decoder\n",
    "        self.AE = None  # Auto encoder model\n",
    "        \n",
    "    def encoder(self):\n",
    "        if self.E:\n",
    "            return self.E\n",
    "        self.E = Sequential()\n",
    "        depth = 64\n",
    "        dropout = 0.6\n",
    "        # In: 64 x 64 x 1, depth = 1\n",
    "        # Out: 64 x 64 x 1, depth=64\n",
    "        input_shape = (self.img_rows, self.img_cols, self.channel)\n",
    "        self.E.add(Conv2D(depth*1, 3, strides=1, input_shape=input_shape,\\\n",
    "            padding='same'))\n",
    "        self.E.add(LeakyReLU(alpha=0.2))\n",
    "        self.E.add(Dropout(dropout))\n",
    "\n",
    "        self.E.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "        self.E.add(BatchNormalization(momentum=0.9))\n",
    "        self.E.add(LeakyReLU(alpha=0.2))\n",
    "        self.E.add(Dropout(dropout))\n",
    "\n",
    "        self.E.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "        self.E.add(BatchNormalization(momentum=0.9))\n",
    "        self.E.add(LeakyReLU(alpha=0.2))\n",
    "        self.E.add(Dropout(dropout))\n",
    "\n",
    "        self.E.summary()\n",
    "        return self.E\n",
    "    \n",
    "    def decoder(self):\n",
    "        if self.D:\n",
    "            return self.D\n",
    "        self.D = Sequential()\n",
    "        dropout = 0.6\n",
    "        depth = 64*4\n",
    "        dim = 16\n",
    "        # In: 100\n",
    "        # Out: dim x dim x depth\n",
    "    \n",
    "        # In: dim x dim x depth\n",
    "        # Out: 2*dim x 2*dim x depth/2\n",
    "        self.D.add(UpSampling2D(input_shape=(16, 16, 256)))\n",
    "        self.D.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "        self.D.add(BatchNormalization(momentum=0.9))\n",
    "        self.D.add(Activation('relu'))\n",
    "\n",
    "        self.D.add(UpSampling2D(size=(2,2)))\n",
    "        self.D.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "        self.D.add(BatchNormalization(momentum=0.9))\n",
    "        self.D.add(Activation('relu'))\n",
    "        \n",
    "        # Out: 256 x 256 x 1 grayscale image [0.0,1.0] per pix\n",
    "        self.D.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "        self.D.add(Activation('tanh'))\n",
    "        \n",
    "        self.D.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "        self.D.add(Activation('linear'))\n",
    "        self.D.summary()\n",
    "        return self.D\n",
    "    \n",
    "    def autoencoder_model(self):\n",
    "        if self.AE:\n",
    "            return self.AE\n",
    "        optimizer = RMSprop(lr=0.0001, decay=3e-8)\n",
    "        self.AE = Sequential()\n",
    "        self.AE.add(self.encoder())\n",
    "        self.AE.add(self.decoder())\n",
    "        self.AE.compile(loss='binary_crossentropy', optimizer=optimizer,\\\n",
    "            metrics=['accuracy'])\n",
    "        return self.AE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FKSIMS_AE(object):\n",
    "    def __init__(self):\n",
    "        self.img_rows = 64\n",
    "        self.img_cols = 64\n",
    "        self.channel = 1\n",
    "\n",
    "        _,self.train_fgs,self.train_mixed = zip(*pk.load(open('Fake_sims_for_separation_test.pk','rb')))\n",
    "        self.train_mixed=np.array(self.train_mixed).reshape(-1, self.img_rows,\\\n",
    "            self.img_cols, 1).astype(np.float32)\n",
    "        self.train_fgs=np.array(self.train_fgs).reshape(-1, self.img_rows,\\\n",
    "            self.img_cols, 1).astype(np.float32)\n",
    "        \n",
    "        self.AE = Autoencoder()\n",
    "        self.autoencoder = self.AE.autoencoder_model()\n",
    "\n",
    "    def train(self, train_steps=2000, batch_size=25, save_interval=0):\n",
    "        for i in range(train_steps):\n",
    "            slice = np.random.randint(0,self.train_mixed.shape[0], size=batch_size)\n",
    "            tmix = self.train_mixed[slice, :, :, :]\n",
    "            tfgs = self.train_fgs[slice, :, :, :]\n",
    "            ae_loss = self.autoencoder.train_on_batch(tmix, tfgs)\n",
    "            log_mesg = \"%d  [AE loss: %f, acc: %f]\" % (i, ae_loss[0], ae_loss[1])\n",
    "            print(log_mesg)\n",
    "            if save_interval>0:\n",
    "                if (i+1)%save_interval==0:\n",
    "                    self.plot_images(tmix, tfgs, save2file=True, samples=8,  step=(i+1))\n",
    "\n",
    "    def plot_images(self, inputs, truths, save2file=False, samples=8, step=0):\n",
    "        filename = \"fake_sims_%d.png\" % step\n",
    "        inputs = inputs[:samples*2]\n",
    "        truths = truths[:samples*2]\n",
    "        fakes = self.autoencoder.predict(inputs)\n",
    "        \n",
    "\n",
    "        plt.figure(figsize=(10,10))\n",
    "        for i in range(samples*2):\n",
    "            if (i+1)%2!=0:\n",
    "                plt.subplot(4, 4, i+1)\n",
    "                truth = truths[i, :, :, :]\n",
    "                truth = np.reshape(truth, [self.img_rows, self.img_cols])\n",
    "                plt.imshow(truth, cmap='gray')\n",
    "                plt.axis('off')\n",
    "            else:\n",
    "                plt.subplot(4, 4, i+1)\n",
    "                fake = fakes[i-1, :, :, :]\n",
    "                fake = np.reshape(fake, [self.img_rows, self.img_cols])\n",
    "                plt.imshow(fake, cmap='gray')\n",
    "                plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        if save2file:\n",
    "            plt.savefig(filename)\n",
    "            plt.close('all')\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_70 (Conv2D)           (None, 64, 64, 64)        640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_71 (Conv2D)           (None, 32, 32, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_88 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_72 (Conv2D)           (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "batch_normalization_89 (Batc (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 16, 16, 256)       0         \n",
      "=================================================================\n",
      "Total params: 1,026,560\n",
      "Trainable params: 1,025,792\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "up_sampling2d_43 (UpSampling (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_82 (Conv2DT (None, 32, 32, 128)       819328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_90 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_44 (UpSampling (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_83 (Conv2DT (None, 64, 64, 64)        204864    \n",
      "_________________________________________________________________\n",
      "batch_normalization_91 (Batc (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_84 (Conv2DT (None, 64, 64, 32)        51232     \n",
      "_________________________________________________________________\n",
      "activation_83 (Activation)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_85 (Conv2DT (None, 64, 64, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_84 (Activation)   (None, 64, 64, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,076,993\n",
      "Trainable params: 1,076,609\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "0  [AE loss: 1.825519, acc: 0.068872]\n",
      "1  [AE loss: 1.457242, acc: 0.056169]\n",
      "2  [AE loss: 1.446975, acc: 0.054888]\n",
      "3  [AE loss: 1.394991, acc: 0.054789]\n",
      "4  [AE loss: 1.443826, acc: 0.060250]\n",
      "5  [AE loss: 1.438397, acc: 0.059507]\n",
      "6  [AE loss: 1.448978, acc: 0.059269]\n",
      "7  [AE loss: 1.404118, acc: 0.070488]\n",
      "8  [AE loss: 1.390730, acc: 0.070762]\n",
      "9  [AE loss: 1.387081, acc: 0.069281]\n",
      "10  [AE loss: 1.395822, acc: 0.069254]\n",
      "11  [AE loss: 1.390795, acc: 0.068428]\n",
      "12  [AE loss: 1.386922, acc: 0.071703]\n",
      "13  [AE loss: 1.468323, acc: 0.071649]\n",
      "14  [AE loss: 1.387933, acc: 0.069442]\n",
      "15  [AE loss: 1.381671, acc: 0.068990]\n",
      "16  [AE loss: 1.424415, acc: 0.042150]\n",
      "17  [AE loss: 1.430514, acc: 0.043239]\n",
      "18  [AE loss: 1.421929, acc: 0.043220]\n",
      "19  [AE loss: 1.436678, acc: 0.045227]\n",
      "20  [AE loss: 1.419173, acc: 0.046243]\n",
      "21  [AE loss: 1.416495, acc: 0.049320]\n",
      "22  [AE loss: 1.399899, acc: 0.052306]\n",
      "23  [AE loss: 1.389996, acc: 0.053669]\n",
      "24  [AE loss: 1.391722, acc: 0.055173]\n",
      "25  [AE loss: 1.383395, acc: 0.056057]\n",
      "26  [AE loss: 1.388579, acc: 0.055942]\n",
      "27  [AE loss: 1.366003, acc: 0.060466]\n",
      "28  [AE loss: 1.364633, acc: 0.062008]\n",
      "29  [AE loss: 1.388734, acc: 0.059800]\n",
      "30  [AE loss: 1.364159, acc: 0.059447]\n",
      "31  [AE loss: 1.388674, acc: 0.061371]\n",
      "32  [AE loss: 1.358950, acc: 0.065443]\n",
      "33  [AE loss: 1.367543, acc: 0.064630]\n",
      "34  [AE loss: 1.368053, acc: 0.068909]\n",
      "35  [AE loss: 1.353112, acc: 0.068838]\n",
      "36  [AE loss: 1.347238, acc: 0.067400]\n",
      "37  [AE loss: 1.351924, acc: 0.066671]\n",
      "38  [AE loss: 1.344661, acc: 0.066918]\n",
      "39  [AE loss: 1.352146, acc: 0.069122]\n",
      "40  [AE loss: 1.363481, acc: 0.061639]\n",
      "41  [AE loss: 1.372917, acc: 0.074611]\n",
      "42  [AE loss: 1.356060, acc: 0.071462]\n",
      "43  [AE loss: 1.342507, acc: 0.071995]\n",
      "44  [AE loss: 1.502119, acc: 0.074000]\n",
      "45  [AE loss: 1.325142, acc: 0.068392]\n",
      "46  [AE loss: 1.316939, acc: 0.068158]\n",
      "47  [AE loss: 1.322924, acc: 0.069204]\n",
      "48  [AE loss: 1.316152, acc: 0.067053]\n",
      "49  [AE loss: 1.316406, acc: 0.067930]\n",
      "50  [AE loss: 1.311809, acc: 0.072141]\n",
      "51  [AE loss: 1.307124, acc: 0.070022]\n",
      "52  [AE loss: 1.323801, acc: 0.073267]\n",
      "53  [AE loss: 1.293832, acc: 0.074071]\n",
      "54  [AE loss: 1.276366, acc: 0.074589]\n",
      "55  [AE loss: 1.272074, acc: 0.073309]\n",
      "56  [AE loss: 1.295630, acc: 0.074578]\n",
      "57  [AE loss: 1.258741, acc: 0.073577]\n",
      "58  [AE loss: 1.351335, acc: 0.056976]\n",
      "59  [AE loss: 1.327695, acc: 0.060201]\n",
      "60  [AE loss: 1.340040, acc: 0.060337]\n",
      "61  [AE loss: 1.298539, acc: 0.063074]\n",
      "62  [AE loss: 1.332809, acc: 0.050986]\n",
      "63  [AE loss: 1.307712, acc: 0.054052]\n",
      "64  [AE loss: 1.296249, acc: 0.058433]\n",
      "65  [AE loss: 1.333637, acc: 0.054796]\n",
      "66  [AE loss: 1.291478, acc: 0.056002]\n",
      "67  [AE loss: 1.276465, acc: 0.061975]\n",
      "68  [AE loss: 1.246210, acc: 0.063430]\n",
      "69  [AE loss: 1.243561, acc: 0.065922]\n",
      "70  [AE loss: 1.243083, acc: 0.073744]\n",
      "71  [AE loss: 1.342641, acc: 0.076460]\n",
      "72  [AE loss: 1.218724, acc: 0.070682]\n",
      "73  [AE loss: 1.258286, acc: 0.073878]\n",
      "74  [AE loss: 1.238795, acc: 0.071741]\n",
      "75  [AE loss: 1.249865, acc: 0.072520]\n",
      "76  [AE loss: 1.225082, acc: 0.076029]\n",
      "77  [AE loss: 1.271969, acc: 0.076760]\n",
      "78  [AE loss: 1.205165, acc: 0.073413]\n",
      "79  [AE loss: 1.193640, acc: 0.075355]\n",
      "80  [AE loss: 1.219376, acc: 0.074899]\n",
      "81  [AE loss: 1.204236, acc: 0.076008]\n",
      "82  [AE loss: 1.196684, acc: 0.075778]\n",
      "83  [AE loss: 1.218157, acc: 0.075349]\n",
      "84  [AE loss: 1.186072, acc: 0.077157]\n",
      "85  [AE loss: 1.269501, acc: 0.073811]\n",
      "86  [AE loss: 1.192991, acc: 0.076014]\n",
      "87  [AE loss: 1.167714, acc: 0.077759]\n",
      "88  [AE loss: 1.191306, acc: 0.076492]\n",
      "89  [AE loss: 1.178196, acc: 0.079056]\n",
      "90  [AE loss: 1.148538, acc: 0.077731]\n",
      "91  [AE loss: 1.129347, acc: 0.077452]\n",
      "92  [AE loss: 1.145796, acc: 0.077821]\n",
      "93  [AE loss: 1.267621, acc: 0.074493]\n",
      "94  [AE loss: 1.215737, acc: 0.075967]\n",
      "95  [AE loss: 1.160338, acc: 0.079094]\n",
      "96  [AE loss: 1.120930, acc: 0.078821]\n",
      "97  [AE loss: 1.131744, acc: 0.079082]\n",
      "98  [AE loss: 1.124236, acc: 0.079545]\n",
      "99  [AE loss: 1.097314, acc: 0.079207]\n",
      "100  [AE loss: 1.101381, acc: 0.079297]\n",
      "101  [AE loss: 1.086599, acc: 0.078698]\n",
      "102  [AE loss: 1.109735, acc: 0.079646]\n",
      "103  [AE loss: 1.087726, acc: 0.077885]\n",
      "104  [AE loss: 1.077120, acc: 0.079191]\n",
      "105  [AE loss: 1.336732, acc: 0.072908]\n",
      "106  [AE loss: 1.275353, acc: 0.075400]\n",
      "107  [AE loss: 1.220518, acc: 0.078236]\n",
      "108  [AE loss: 1.219560, acc: 0.078894]\n",
      "109  [AE loss: 1.167978, acc: 0.079586]\n",
      "110  [AE loss: 1.156823, acc: 0.078977]\n",
      "111  [AE loss: 1.076965, acc: 0.079926]\n",
      "112  [AE loss: 1.094846, acc: 0.080524]\n",
      "113  [AE loss: 1.090649, acc: 0.079381]\n",
      "114  [AE loss: 1.073627, acc: 0.079904]\n",
      "115  [AE loss: 1.068670, acc: 0.079869]\n",
      "116  [AE loss: 1.057850, acc: 0.080454]\n",
      "117  [AE loss: 1.067904, acc: 0.079458]\n",
      "118  [AE loss: 1.140679, acc: 0.078851]\n",
      "119  [AE loss: 1.024875, acc: 0.080768]\n",
      "120  [AE loss: 1.039004, acc: 0.079950]\n",
      "121  [AE loss: 1.033859, acc: 0.080215]\n",
      "122  [AE loss: 1.036464, acc: 0.080570]\n",
      "123  [AE loss: 1.017001, acc: 0.080947]\n",
      "124  [AE loss: 1.014819, acc: 0.079629]\n",
      "125  [AE loss: 1.135783, acc: 0.078938]\n",
      "126  [AE loss: 1.029507, acc: 0.080077]\n",
      "127  [AE loss: 1.014548, acc: 0.080084]\n",
      "128  [AE loss: 1.002720, acc: 0.080126]\n",
      "129  [AE loss: 1.005740, acc: 0.080394]\n",
      "130  [AE loss: 0.992641, acc: 0.081094]\n",
      "131  [AE loss: 0.979025, acc: 0.080927]\n",
      "132  [AE loss: 1.075977, acc: 0.078096]\n",
      "133  [AE loss: 0.989008, acc: 0.080154]\n",
      "134  [AE loss: 1.002231, acc: 0.079620]\n",
      "135  [AE loss: 0.978336, acc: 0.081198]\n",
      "136  [AE loss: 0.939384, acc: 0.081774]\n",
      "137  [AE loss: 0.963366, acc: 0.080402]\n",
      "138  [AE loss: 0.953197, acc: 0.080824]\n",
      "139  [AE loss: 0.942800, acc: 0.080769]\n",
      "140  [AE loss: 0.918059, acc: 0.081123]\n",
      "141  [AE loss: 0.927138, acc: 0.080806]\n",
      "142  [AE loss: 1.320943, acc: 0.072993]\n",
      "143  [AE loss: 0.936350, acc: 0.082043]\n",
      "144  [AE loss: 0.980414, acc: 0.080868]\n",
      "145  [AE loss: 0.862521, acc: 0.081378]\n",
      "146  [AE loss: 0.867003, acc: 0.081705]\n",
      "147  [AE loss: 0.862734, acc: 0.082131]\n",
      "148  [AE loss: 0.852602, acc: 0.081570]\n",
      "149  [AE loss: 0.844432, acc: 0.081964]\n",
      "150  [AE loss: 0.828497, acc: 0.081484]\n",
      "151  [AE loss: 0.843405, acc: 0.081311]\n",
      "152  [AE loss: 0.813931, acc: 0.081863]\n",
      "153  [AE loss: 0.802016, acc: 0.082129]\n",
      "154  [AE loss: 0.789659, acc: 0.082040]\n",
      "155  [AE loss: 0.777554, acc: 0.081952]\n",
      "156  [AE loss: 0.783650, acc: 0.082219]\n",
      "157  [AE loss: 0.952089, acc: 0.080396]\n",
      "158  [AE loss: 0.770149, acc: 0.081821]\n",
      "159  [AE loss: 0.770758, acc: 0.081665]\n",
      "160  [AE loss: 0.766583, acc: 0.081650]\n",
      "161  [AE loss: 0.743994, acc: 0.082521]\n",
      "162  [AE loss: 0.726884, acc: 0.081997]\n",
      "163  [AE loss: 0.860982, acc: 0.081136]\n",
      "164  [AE loss: 0.729547, acc: 0.082118]\n",
      "165  [AE loss: 0.727126, acc: 0.081740]\n",
      "166  [AE loss: 0.735544, acc: 0.082383]\n",
      "167  [AE loss: 0.684205, acc: 0.082854]\n",
      "168  [AE loss: 0.666392, acc: 0.082603]\n",
      "169  [AE loss: 0.652465, acc: 0.082421]\n",
      "170  [AE loss: 0.658338, acc: 0.081501]\n",
      "171  [AE loss: 0.722537, acc: 0.081002]\n",
      "172  [AE loss: 0.674330, acc: 0.082371]\n",
      "173  [AE loss: 0.640639, acc: 0.082238]\n",
      "174  [AE loss: 0.615138, acc: 0.082495]\n",
      "175  [AE loss: 0.794304, acc: 0.083092]\n",
      "176  [AE loss: 0.731614, acc: 0.080624]\n",
      "177  [AE loss: 0.606620, acc: 0.082253]\n",
      "178  [AE loss: 0.593278, acc: 0.082556]\n",
      "179  [AE loss: 0.594811, acc: 0.082400]\n",
      "180  [AE loss: 0.594072, acc: 0.081793]\n",
      "181  [AE loss: 0.578487, acc: 0.082109]\n",
      "182  [AE loss: 0.720630, acc: 0.081700]\n",
      "183  [AE loss: 0.620247, acc: 0.082034]\n",
      "184  [AE loss: 0.578209, acc: 0.082020]\n",
      "185  [AE loss: 0.570703, acc: 0.082877]\n",
      "186  [AE loss: 0.565006, acc: 0.082147]\n",
      "187  [AE loss: 0.551675, acc: 0.082947]\n",
      "188  [AE loss: 0.539100, acc: 0.082485]\n",
      "189  [AE loss: 0.585139, acc: 0.082108]\n",
      "190  [AE loss: 0.553401, acc: 0.082806]\n",
      "191  [AE loss: 0.555501, acc: 0.081251]\n",
      "192  [AE loss: 0.601941, acc: 0.082178]\n",
      "193  [AE loss: 0.558306, acc: 0.082039]\n",
      "194  [AE loss: 0.541230, acc: 0.081608]\n",
      "195  [AE loss: 0.545540, acc: 0.082731]\n",
      "196  [AE loss: 0.515517, acc: 0.081426]\n",
      "197  [AE loss: 0.528565, acc: 0.082347]\n",
      "198  [AE loss: 0.502126, acc: 0.083425]\n",
      "199  [AE loss: 0.515385, acc: 0.082047]\n",
      "200  [AE loss: 0.535757, acc: 0.082269]\n",
      "201  [AE loss: 0.544537, acc: 0.082120]\n",
      "202  [AE loss: 0.608861, acc: 0.081959]\n",
      "203  [AE loss: 0.523893, acc: 0.082581]\n",
      "204  [AE loss: 0.556980, acc: 0.082808]\n",
      "205  [AE loss: 0.501360, acc: 0.082651]\n",
      "206  [AE loss: 0.506690, acc: 0.082195]\n",
      "207  [AE loss: 0.506868, acc: 0.082805]\n",
      "208  [AE loss: 0.556936, acc: 0.081720]\n",
      "209  [AE loss: 0.470352, acc: 0.082523]\n",
      "210  [AE loss: 0.453181, acc: 0.082787]\n",
      "211  [AE loss: 0.450520, acc: 0.082909]\n",
      "212  [AE loss: 0.450006, acc: 0.082373]\n",
      "213  [AE loss: 0.448312, acc: 0.083466]\n",
      "214  [AE loss: 0.441573, acc: 0.081692]\n",
      "215  [AE loss: 0.458775, acc: 0.083202]\n",
      "216  [AE loss: 0.643614, acc: 0.080847]\n",
      "217  [AE loss: 0.462657, acc: 0.082174]\n",
      "218  [AE loss: 0.428271, acc: 0.083716]\n",
      "219  [AE loss: 0.427911, acc: 0.081895]\n",
      "220  [AE loss: 0.454904, acc: 0.082654]\n",
      "221  [AE loss: 0.443058, acc: 0.082135]\n",
      "222  [AE loss: 0.468248, acc: 0.083107]\n",
      "223  [AE loss: 0.432122, acc: 0.083611]\n",
      "224  [AE loss: 0.459936, acc: 0.082048]\n",
      "225  [AE loss: 0.452358, acc: 0.082250]\n",
      "226  [AE loss: 0.498809, acc: 0.082380]\n",
      "227  [AE loss: 0.421197, acc: 0.082567]\n",
      "228  [AE loss: 0.408714, acc: 0.083157]\n",
      "229  [AE loss: 0.411774, acc: 0.082443]\n",
      "230  [AE loss: 0.523731, acc: 0.082028]\n",
      "231  [AE loss: 0.412512, acc: 0.082491]\n",
      "232  [AE loss: 0.415463, acc: 0.082765]\n",
      "233  [AE loss: 0.414021, acc: 0.081832]\n",
      "234  [AE loss: 0.433056, acc: 0.081973]\n",
      "235  [AE loss: 0.412863, acc: 0.083362]\n",
      "236  [AE loss: 0.426565, acc: 0.083188]\n",
      "237  [AE loss: 0.410719, acc: 0.082295]\n",
      "238  [AE loss: 0.457195, acc: 0.082498]\n",
      "239  [AE loss: 0.399939, acc: 0.082821]\n",
      "240  [AE loss: 0.400219, acc: 0.083677]\n",
      "241  [AE loss: 0.401173, acc: 0.082032]\n",
      "242  [AE loss: 0.497077, acc: 0.083011]\n",
      "243  [AE loss: 0.378960, acc: 0.082180]\n",
      "244  [AE loss: 0.362851, acc: 0.083347]\n",
      "245  [AE loss: 0.361757, acc: 0.083612]\n",
      "246  [AE loss: 0.370013, acc: 0.083208]\n",
      "247  [AE loss: 0.397891, acc: 0.082067]\n",
      "248  [AE loss: 0.389645, acc: 0.082976]\n",
      "249  [AE loss: 0.543238, acc: 0.082666]\n",
      "250  [AE loss: 0.381225, acc: 0.082845]\n",
      "251  [AE loss: 0.361209, acc: 0.083295]\n",
      "252  [AE loss: 0.364232, acc: 0.082924]\n",
      "253  [AE loss: 0.386479, acc: 0.083168]\n",
      "254  [AE loss: 0.380305, acc: 0.082389]\n",
      "255  [AE loss: 0.451406, acc: 0.082485]\n",
      "256  [AE loss: 0.353691, acc: 0.082097]\n",
      "257  [AE loss: 0.357610, acc: 0.082860]\n",
      "258  [AE loss: 0.344016, acc: 0.083008]\n",
      "259  [AE loss: 0.335912, acc: 0.083795]\n",
      "260  [AE loss: 0.350743, acc: 0.082510]\n",
      "261  [AE loss: 0.460063, acc: 0.082573]\n",
      "262  [AE loss: 0.346073, acc: 0.082684]\n",
      "263  [AE loss: 0.350020, acc: 0.083821]\n",
      "264  [AE loss: 0.393837, acc: 0.082000]\n",
      "265  [AE loss: 0.625985, acc: 0.080352]\n",
      "266  [AE loss: 0.366677, acc: 0.082327]\n",
      "267  [AE loss: 0.334931, acc: 0.082606]\n",
      "268  [AE loss: 0.328319, acc: 0.082980]\n",
      "269  [AE loss: 0.329025, acc: 0.083174]\n",
      "270  [AE loss: 0.342784, acc: 0.082571]\n",
      "271  [AE loss: 0.400769, acc: 0.083253]\n",
      "272  [AE loss: 0.332416, acc: 0.083475]\n",
      "273  [AE loss: 0.350614, acc: 0.082789]\n",
      "274  [AE loss: 0.383295, acc: 0.083066]\n",
      "275  [AE loss: 0.609694, acc: 0.080667]\n",
      "276  [AE loss: 0.382361, acc: 0.082368]\n",
      "277  [AE loss: 0.328791, acc: 0.082534]\n",
      "278  [AE loss: 0.321903, acc: 0.082379]\n",
      "279  [AE loss: 0.324146, acc: 0.082814]\n",
      "280  [AE loss: 0.378261, acc: 0.082751]\n",
      "281  [AE loss: 0.323897, acc: 0.082769]\n",
      "282  [AE loss: 0.325029, acc: 0.082518]\n",
      "283  [AE loss: 0.336702, acc: 0.083038]\n",
      "284  [AE loss: 0.390630, acc: 0.083345]\n",
      "285  [AE loss: 0.324504, acc: 0.083445]\n",
      "286  [AE loss: 0.317606, acc: 0.082789]\n",
      "287  [AE loss: 0.461895, acc: 0.082338]\n",
      "288  [AE loss: 0.335687, acc: 0.082003]\n",
      "289  [AE loss: 0.317456, acc: 0.083411]\n",
      "290  [AE loss: 0.362372, acc: 0.082544]\n",
      "291  [AE loss: 0.322824, acc: 0.082588]\n",
      "292  [AE loss: 0.349995, acc: 0.082577]\n",
      "293  [AE loss: 0.357636, acc: 0.082296]\n",
      "294  [AE loss: 0.660354, acc: 0.079019]\n",
      "295  [AE loss: 0.415118, acc: 0.083368]\n",
      "296  [AE loss: 0.347235, acc: 0.083048]\n",
      "297  [AE loss: 0.322292, acc: 0.082732]\n",
      "298  [AE loss: 0.315361, acc: 0.082574]\n",
      "299  [AE loss: 0.312156, acc: 0.083923]\n",
      "300  [AE loss: 0.313053, acc: 0.082213]\n",
      "301  [AE loss: 0.313093, acc: 0.083335]\n",
      "302  [AE loss: 0.317952, acc: 0.082460]\n",
      "303  [AE loss: 0.325767, acc: 0.082944]\n",
      "304  [AE loss: 0.325353, acc: 0.082427]\n",
      "305  [AE loss: 0.402753, acc: 0.081835]\n",
      "306  [AE loss: 0.317259, acc: 0.083151]\n",
      "307  [AE loss: 0.312708, acc: 0.083573]\n",
      "308  [AE loss: 0.323226, acc: 0.082380]\n",
      "309  [AE loss: 0.319830, acc: 0.082714]\n",
      "310  [AE loss: 0.376375, acc: 0.082950]\n",
      "311  [AE loss: 0.316601, acc: 0.082831]\n",
      "312  [AE loss: 0.321001, acc: 0.082375]\n",
      "313  [AE loss: 0.341088, acc: 0.082388]\n",
      "314  [AE loss: 0.590289, acc: 0.079769]\n",
      "315  [AE loss: 0.371530, acc: 0.083065]\n",
      "316  [AE loss: 0.314965, acc: 0.083347]\n",
      "317  [AE loss: 0.311770, acc: 0.083230]\n",
      "318  [AE loss: 0.311028, acc: 0.082743]\n",
      "319  [AE loss: 0.312391, acc: 0.082448]\n",
      "320  [AE loss: 0.358537, acc: 0.081904]\n",
      "321  [AE loss: 0.311278, acc: 0.082072]\n",
      "322  [AE loss: 0.308273, acc: 0.082461]\n",
      "323  [AE loss: 0.309193, acc: 0.082438]\n",
      "324  [AE loss: 0.308624, acc: 0.082509]\n",
      "325  [AE loss: 0.316115, acc: 0.083375]\n",
      "326  [AE loss: 0.345750, acc: 0.082371]\n",
      "327  [AE loss: 0.643946, acc: 0.078398]\n",
      "328  [AE loss: 0.397108, acc: 0.082792]\n",
      "329  [AE loss: 0.333654, acc: 0.082479]\n",
      "330  [AE loss: 0.329497, acc: 0.083195]\n",
      "331  [AE loss: 0.314140, acc: 0.081748]\n",
      "332  [AE loss: 0.309675, acc: 0.083492]\n",
      "333  [AE loss: 0.310977, acc: 0.082889]\n",
      "334  [AE loss: 0.315086, acc: 0.081483]\n",
      "335  [AE loss: 0.307643, acc: 0.082430]\n",
      "336  [AE loss: 0.315909, acc: 0.082532]\n",
      "337  [AE loss: 0.315337, acc: 0.082938]\n",
      "338  [AE loss: 0.374519, acc: 0.083390]\n",
      "339  [AE loss: 0.306872, acc: 0.083351]\n",
      "340  [AE loss: 0.311747, acc: 0.082938]\n",
      "341  [AE loss: 0.320192, acc: 0.082432]\n",
      "342  [AE loss: 0.467498, acc: 0.082184]\n",
      "343  [AE loss: 0.326978, acc: 0.082572]\n",
      "344  [AE loss: 0.306683, acc: 0.083243]\n",
      "345  [AE loss: 0.308384, acc: 0.081772]\n",
      "346  [AE loss: 0.323326, acc: 0.082749]\n",
      "347  [AE loss: 0.487156, acc: 0.083616]\n",
      "348  [AE loss: 0.335398, acc: 0.082076]\n",
      "349  [AE loss: 0.305791, acc: 0.083346]\n",
      "350  [AE loss: 0.303447, acc: 0.082416]\n",
      "351  [AE loss: 0.304144, acc: 0.083235]\n",
      "352  [AE loss: 0.311739, acc: 0.082743]\n",
      "353  [AE loss: 0.376218, acc: 0.081697]\n",
      "354  [AE loss: 0.303598, acc: 0.082480]\n",
      "355  [AE loss: 0.302091, acc: 0.082992]\n",
      "356  [AE loss: 0.320133, acc: 0.083260]\n",
      "357  [AE loss: 0.313159, acc: 0.083113]\n",
      "358  [AE loss: 0.414224, acc: 0.081914]\n",
      "359  [AE loss: 0.305287, acc: 0.082476]\n",
      "360  [AE loss: 0.302488, acc: 0.083042]\n",
      "361  [AE loss: 0.326570, acc: 0.082833]\n",
      "362  [AE loss: 0.298512, acc: 0.082307]\n",
      "363  [AE loss: 0.304417, acc: 0.082622]\n",
      "364  [AE loss: 0.382262, acc: 0.083068]\n",
      "365  [AE loss: 0.676961, acc: 0.077889]\n",
      "366  [AE loss: 0.469501, acc: 0.082670]\n",
      "367  [AE loss: 0.384005, acc: 0.082313]\n",
      "368  [AE loss: 0.337330, acc: 0.082516]\n",
      "369  [AE loss: 0.314900, acc: 0.082982]\n",
      "370  [AE loss: 0.302907, acc: 0.082697]\n",
      "371  [AE loss: 0.303275, acc: 0.083379]\n",
      "372  [AE loss: 0.301876, acc: 0.083590]\n",
      "373  [AE loss: 0.302636, acc: 0.083422]\n",
      "374  [AE loss: 0.300180, acc: 0.082313]\n",
      "375  [AE loss: 0.298279, acc: 0.081775]\n",
      "376  [AE loss: 0.297927, acc: 0.082609]\n",
      "377  [AE loss: 0.305881, acc: 0.082930]\n",
      "378  [AE loss: 0.303994, acc: 0.082577]\n",
      "379  [AE loss: 0.316323, acc: 0.083303]\n",
      "380  [AE loss: 0.301174, acc: 0.082744]\n",
      "381  [AE loss: 0.310115, acc: 0.082500]\n",
      "382  [AE loss: 0.308014, acc: 0.082839]\n",
      "383  [AE loss: 0.398052, acc: 0.082815]\n",
      "384  [AE loss: 0.317038, acc: 0.082588]\n",
      "385  [AE loss: 0.307805, acc: 0.083212]\n",
      "386  [AE loss: 0.300710, acc: 0.083745]\n",
      "387  [AE loss: 0.305947, acc: 0.082828]\n",
      "388  [AE loss: 0.307510, acc: 0.083126]\n",
      "389  [AE loss: 0.388157, acc: 0.082556]\n",
      "390  [AE loss: 0.302906, acc: 0.082917]\n",
      "391  [AE loss: 0.299901, acc: 0.082085]\n",
      "392  [AE loss: 0.309875, acc: 0.083450]\n",
      "393  [AE loss: 0.317888, acc: 0.081964]\n",
      "394  [AE loss: 0.492436, acc: 0.083733]\n",
      "395  [AE loss: 0.332880, acc: 0.082903]\n",
      "396  [AE loss: 0.302833, acc: 0.083059]\n",
      "397  [AE loss: 0.298277, acc: 0.082793]\n",
      "398  [AE loss: 0.298681, acc: 0.083458]\n",
      "399  [AE loss: 0.303763, acc: 0.082732]\n",
      "400  [AE loss: 0.331131, acc: 0.083243]\n",
      "401  [AE loss: 0.298896, acc: 0.083317]\n",
      "402  [AE loss: 0.299684, acc: 0.083428]\n",
      "403  [AE loss: 0.296542, acc: 0.083005]\n",
      "404  [AE loss: 0.302175, acc: 0.081346]\n",
      "405  [AE loss: 0.402135, acc: 0.083052]\n",
      "406  [AE loss: 0.302090, acc: 0.083193]\n",
      "407  [AE loss: 0.303429, acc: 0.083093]\n",
      "408  [AE loss: 0.298098, acc: 0.082968]\n",
      "409  [AE loss: 0.295851, acc: 0.083706]\n",
      "410  [AE loss: 0.300246, acc: 0.083020]\n",
      "411  [AE loss: 0.309420, acc: 0.082476]\n",
      "412  [AE loss: 0.457244, acc: 0.082471]\n",
      "413  [AE loss: 0.670707, acc: 0.077711]\n",
      "414  [AE loss: 0.539332, acc: 0.082102]\n",
      "415  [AE loss: 0.465277, acc: 0.082098]\n",
      "416  [AE loss: 0.413768, acc: 0.082203]\n",
      "417  [AE loss: 0.374530, acc: 0.083254]\n",
      "418  [AE loss: 0.346011, acc: 0.081781]\n",
      "419  [AE loss: 0.322079, acc: 0.083082]\n",
      "420  [AE loss: 0.315448, acc: 0.083140]\n",
      "421  [AE loss: 0.307370, acc: 0.082722]\n",
      "422  [AE loss: 0.300396, acc: 0.083546]\n",
      "423  [AE loss: 0.296385, acc: 0.082850]\n",
      "424  [AE loss: 0.295774, acc: 0.083516]\n",
      "425  [AE loss: 0.294306, acc: 0.083196]\n",
      "426  [AE loss: 0.297765, acc: 0.083112]\n",
      "427  [AE loss: 0.297205, acc: 0.082395]\n",
      "428  [AE loss: 0.293748, acc: 0.082989]\n",
      "429  [AE loss: 0.296315, acc: 0.081986]\n",
      "430  [AE loss: 0.293669, acc: 0.083207]\n",
      "431  [AE loss: 0.295995, acc: 0.082603]\n",
      "432  [AE loss: 0.295481, acc: 0.081956]\n",
      "433  [AE loss: 0.296408, acc: 0.082216]\n",
      "434  [AE loss: 0.296054, acc: 0.083542]\n",
      "435  [AE loss: 0.301442, acc: 0.082029]\n",
      "436  [AE loss: 0.315885, acc: 0.083058]\n",
      "437  [AE loss: 0.296792, acc: 0.083463]\n",
      "438  [AE loss: 0.293068, acc: 0.083013]\n",
      "439  [AE loss: 0.295676, acc: 0.082852]\n",
      "440  [AE loss: 0.296366, acc: 0.084017]\n",
      "441  [AE loss: 0.294923, acc: 0.082582]\n",
      "442  [AE loss: 0.322638, acc: 0.082227]\n",
      "443  [AE loss: 0.299567, acc: 0.083387]\n",
      "444  [AE loss: 0.307579, acc: 0.082637]\n",
      "445  [AE loss: 0.319928, acc: 0.082927]\n",
      "446  [AE loss: 0.548346, acc: 0.083176]\n",
      "447  [AE loss: 0.377912, acc: 0.082131]\n",
      "448  [AE loss: 0.313730, acc: 0.082340]\n",
      "449  [AE loss: 0.300714, acc: 0.083318]\n",
      "450  [AE loss: 0.296271, acc: 0.082999]\n",
      "451  [AE loss: 0.300055, acc: 0.082106]\n",
      "452  [AE loss: 0.294944, acc: 0.082437]\n",
      "453  [AE loss: 0.295107, acc: 0.083621]\n",
      "454  [AE loss: 0.292571, acc: 0.083181]\n",
      "455  [AE loss: 0.297964, acc: 0.082894]\n",
      "456  [AE loss: 0.300621, acc: 0.082223]\n",
      "457  [AE loss: 0.344226, acc: 0.082606]\n",
      "458  [AE loss: 0.300227, acc: 0.083455]\n",
      "459  [AE loss: 0.307099, acc: 0.082329]\n",
      "460  [AE loss: 0.319466, acc: 0.082860]\n",
      "461  [AE loss: 0.583661, acc: 0.081765]\n",
      "462  [AE loss: 0.403089, acc: 0.083359]\n",
      "463  [AE loss: 0.333012, acc: 0.082894]\n",
      "464  [AE loss: 0.307451, acc: 0.083157]\n",
      "465  [AE loss: 0.300702, acc: 0.081667]\n",
      "466  [AE loss: 0.298975, acc: 0.082589]\n",
      "467  [AE loss: 0.296153, acc: 0.082866]\n",
      "468  [AE loss: 0.296278, acc: 0.082849]\n",
      "469  [AE loss: 0.298865, acc: 0.082587]\n",
      "470  [AE loss: 0.296795, acc: 0.082660]\n",
      "471  [AE loss: 0.292691, acc: 0.083639]\n",
      "472  [AE loss: 0.295224, acc: 0.082252]\n",
      "473  [AE loss: 0.294373, acc: 0.082433]\n",
      "474  [AE loss: 0.295412, acc: 0.082527]\n",
      "475  [AE loss: 0.299959, acc: 0.083115]\n",
      "476  [AE loss: 0.355117, acc: 0.082957]\n",
      "477  [AE loss: 0.299811, acc: 0.083494]\n",
      "478  [AE loss: 0.302408, acc: 0.083147]\n",
      "479  [AE loss: 0.294598, acc: 0.082739]\n",
      "480  [AE loss: 0.295421, acc: 0.083235]\n",
      "481  [AE loss: 0.316656, acc: 0.083049]\n",
      "482  [AE loss: 0.539371, acc: 0.082540]\n",
      "483  [AE loss: 0.359005, acc: 0.083256]\n",
      "484  [AE loss: 0.306291, acc: 0.082600]\n",
      "485  [AE loss: 0.296582, acc: 0.082657]\n",
      "486  [AE loss: 0.299475, acc: 0.082596]\n",
      "487  [AE loss: 0.298513, acc: 0.082367]\n",
      "488  [AE loss: 0.307333, acc: 0.082683]\n",
      "489  [AE loss: 0.296880, acc: 0.082933]\n",
      "490  [AE loss: 0.297869, acc: 0.082765]\n",
      "491  [AE loss: 0.305869, acc: 0.082352]\n",
      "492  [AE loss: 0.413186, acc: 0.082324]\n",
      "493  [AE loss: 0.311422, acc: 0.082838]\n",
      "494  [AE loss: 0.305690, acc: 0.082743]\n",
      "495  [AE loss: 0.300283, acc: 0.083186]\n",
      "496  [AE loss: 0.340826, acc: 0.082120]\n",
      "497  [AE loss: 0.295485, acc: 0.082832]\n",
      "498  [AE loss: 0.296794, acc: 0.083722]\n",
      "499  [AE loss: 0.300111, acc: 0.082021]\n",
      "500  [AE loss: 0.382197, acc: 0.082708]\n",
      "501  [AE loss: 0.294576, acc: 0.083477]\n",
      "502  [AE loss: 0.294069, acc: 0.082166]\n",
      "503  [AE loss: 0.306450, acc: 0.083024]\n",
      "504  [AE loss: 0.319372, acc: 0.082699]\n",
      "505  [AE loss: 0.532251, acc: 0.082847]\n",
      "506  [AE loss: 0.376998, acc: 0.082644]\n",
      "507  [AE loss: 0.315946, acc: 0.082819]\n",
      "508  [AE loss: 0.294715, acc: 0.083013]\n",
      "509  [AE loss: 0.294147, acc: 0.082708]\n",
      "510  [AE loss: 0.292019, acc: 0.082938]\n",
      "511  [AE loss: 0.291531, acc: 0.083298]\n",
      "512  [AE loss: 0.289771, acc: 0.082113]\n",
      "513  [AE loss: 0.290805, acc: 0.082377]\n",
      "514  [AE loss: 0.291300, acc: 0.082788]\n",
      "515  [AE loss: 0.289525, acc: 0.083221]\n",
      "516  [AE loss: 0.301521, acc: 0.082813]\n",
      "517  [AE loss: 0.399357, acc: 0.083129]\n",
      "518  [AE loss: 0.307507, acc: 0.083062]\n",
      "519  [AE loss: 0.293488, acc: 0.083229]\n",
      "520  [AE loss: 0.295172, acc: 0.082939]\n",
      "521  [AE loss: 0.303219, acc: 0.082433]\n",
      "522  [AE loss: 0.398134, acc: 0.083461]\n",
      "523  [AE loss: 0.303048, acc: 0.082228]\n",
      "524  [AE loss: 0.294215, acc: 0.082201]\n",
      "525  [AE loss: 0.305915, acc: 0.083290]\n",
      "526  [AE loss: 0.300268, acc: 0.082450]\n",
      "527  [AE loss: 0.357888, acc: 0.082537]\n",
      "528  [AE loss: 0.293565, acc: 0.082446]\n",
      "529  [AE loss: 0.292706, acc: 0.082838]\n",
      "530  [AE loss: 0.292985, acc: 0.082931]\n",
      "531  [AE loss: 0.290793, acc: 0.082275]\n",
      "532  [AE loss: 0.292889, acc: 0.082491]\n",
      "533  [AE loss: 0.320071, acc: 0.083899]\n",
      "534  [AE loss: 0.348977, acc: 0.083700]\n",
      "535  [AE loss: 0.555375, acc: 0.083677]\n",
      "536  [AE loss: 0.402777, acc: 0.082557]\n",
      "537  [AE loss: 0.330691, acc: 0.083281]\n",
      "538  [AE loss: 0.301530, acc: 0.082871]\n",
      "539  [AE loss: 0.293218, acc: 0.083010]\n",
      "540  [AE loss: 0.292118, acc: 0.082592]\n",
      "541  [AE loss: 0.293819, acc: 0.083417]\n",
      "542  [AE loss: 0.290446, acc: 0.082377]\n",
      "543  [AE loss: 0.291611, acc: 0.082771]\n",
      "544  [AE loss: 0.292646, acc: 0.082001]\n",
      "545  [AE loss: 0.290075, acc: 0.083279]\n",
      "546  [AE loss: 0.291075, acc: 0.082971]\n",
      "547  [AE loss: 0.291385, acc: 0.082433]\n",
      "548  [AE loss: 0.289073, acc: 0.082417]\n",
      "549  [AE loss: 0.293480, acc: 0.082551]\n",
      "550  [AE loss: 0.294737, acc: 0.082920]\n",
      "551  [AE loss: 0.377867, acc: 0.083214]\n",
      "552  [AE loss: 0.304143, acc: 0.082401]\n",
      "553  [AE loss: 0.296025, acc: 0.082975]\n",
      "554  [AE loss: 0.294494, acc: 0.082827]\n",
      "555  [AE loss: 0.292912, acc: 0.082277]\n",
      "556  [AE loss: 0.307392, acc: 0.083062]\n",
      "557  [AE loss: 0.440402, acc: 0.083700]\n",
      "558  [AE loss: 0.311690, acc: 0.082534]\n",
      "559  [AE loss: 0.291739, acc: 0.083240]\n",
      "560  [AE loss: 0.291104, acc: 0.082339]\n",
      "561  [AE loss: 0.296528, acc: 0.082340]\n",
      "562  [AE loss: 0.312223, acc: 0.082493]\n",
      "563  [AE loss: 0.300740, acc: 0.082754]\n",
      "564  [AE loss: 0.382506, acc: 0.083635]\n",
      "565  [AE loss: 0.297067, acc: 0.082646]\n",
      "566  [AE loss: 0.292271, acc: 0.083398]\n",
      "567  [AE loss: 0.300695, acc: 0.082909]\n",
      "568  [AE loss: 0.303520, acc: 0.082145]\n",
      "569  [AE loss: 0.419425, acc: 0.082767]\n",
      "570  [AE loss: 0.308860, acc: 0.082931]\n",
      "571  [AE loss: 0.291922, acc: 0.082341]\n",
      "572  [AE loss: 0.289674, acc: 0.083206]\n",
      "573  [AE loss: 0.292377, acc: 0.082645]\n",
      "574  [AE loss: 0.293827, acc: 0.082861]\n",
      "575  [AE loss: 0.304089, acc: 0.082834]\n",
      "576  [AE loss: 0.377742, acc: 0.083000]\n",
      "577  [AE loss: 0.297276, acc: 0.082329]\n",
      "578  [AE loss: 0.291724, acc: 0.082617]\n",
      "579  [AE loss: 0.292998, acc: 0.082883]\n",
      "580  [AE loss: 0.296803, acc: 0.082533]\n",
      "581  [AE loss: 0.369860, acc: 0.082451]\n",
      "582  [AE loss: 0.296139, acc: 0.082882]\n",
      "583  [AE loss: 0.291276, acc: 0.082410]\n",
      "584  [AE loss: 0.288616, acc: 0.083071]\n",
      "585  [AE loss: 0.291447, acc: 0.083217]\n",
      "586  [AE loss: 0.292355, acc: 0.083534]\n",
      "587  [AE loss: 0.300438, acc: 0.082606]\n",
      "588  [AE loss: 0.414168, acc: 0.083030]\n",
      "589  [AE loss: 0.303280, acc: 0.082672]\n",
      "590  [AE loss: 0.291205, acc: 0.082485]\n",
      "591  [AE loss: 0.291378, acc: 0.082971]\n",
      "592  [AE loss: 0.296258, acc: 0.082938]\n",
      "593  [AE loss: 0.364614, acc: 0.083159]\n",
      "594  [AE loss: 0.293972, acc: 0.082803]\n",
      "595  [AE loss: 0.290435, acc: 0.081941]\n",
      "596  [AE loss: 0.292539, acc: 0.082343]\n",
      "597  [AE loss: 0.291413, acc: 0.082748]\n",
      "598  [AE loss: 0.293427, acc: 0.082723]\n",
      "599  [AE loss: 0.299066, acc: 0.082473]\n",
      "600  [AE loss: 0.462775, acc: 0.083271]\n",
      "601  [AE loss: 0.323313, acc: 0.082697]\n",
      "602  [AE loss: 0.294631, acc: 0.082634]\n",
      "603  [AE loss: 0.285697, acc: 0.083827]\n",
      "604  [AE loss: 0.291373, acc: 0.083015]\n",
      "605  [AE loss: 0.288547, acc: 0.083132]\n",
      "606  [AE loss: 0.289748, acc: 0.082931]\n",
      "607  [AE loss: 0.291260, acc: 0.082574]\n",
      "608  [AE loss: 0.300826, acc: 0.082411]\n",
      "609  [AE loss: 0.331355, acc: 0.083491]\n",
      "610  [AE loss: 0.512090, acc: 0.082723]\n",
      "611  [AE loss: 0.368750, acc: 0.083411]\n",
      "612  [AE loss: 0.319673, acc: 0.082214]\n",
      "613  [AE loss: 0.294978, acc: 0.083673]\n",
      "614  [AE loss: 0.294301, acc: 0.082778]\n",
      "615  [AE loss: 0.293832, acc: 0.082019]\n",
      "616  [AE loss: 0.294935, acc: 0.082537]\n",
      "617  [AE loss: 0.291246, acc: 0.082501]\n",
      "618  [AE loss: 0.291986, acc: 0.082773]\n",
      "619  [AE loss: 0.290314, acc: 0.082444]\n",
      "620  [AE loss: 0.290424, acc: 0.083351]\n",
      "621  [AE loss: 0.294605, acc: 0.083252]\n",
      "622  [AE loss: 0.298106, acc: 0.082969]\n",
      "623  [AE loss: 0.347490, acc: 0.083413]\n",
      "624  [AE loss: 0.292477, acc: 0.082307]\n",
      "625  [AE loss: 0.289383, acc: 0.083242]\n",
      "626  [AE loss: 0.290631, acc: 0.082692]\n",
      "627  [AE loss: 0.294256, acc: 0.083250]\n",
      "628  [AE loss: 0.322809, acc: 0.082324]\n",
      "629  [AE loss: 0.299162, acc: 0.082704]\n",
      "630  [AE loss: 0.342938, acc: 0.082816]\n",
      "631  [AE loss: 0.288891, acc: 0.082263]\n",
      "632  [AE loss: 0.290875, acc: 0.083062]\n",
      "633  [AE loss: 0.293424, acc: 0.082948]\n",
      "634  [AE loss: 0.298796, acc: 0.082137]\n",
      "635  [AE loss: 0.328014, acc: 0.082585]\n",
      "636  [AE loss: 0.489874, acc: 0.083248]\n",
      "637  [AE loss: 0.354928, acc: 0.083085]\n",
      "638  [AE loss: 0.311602, acc: 0.082838]\n",
      "639  [AE loss: 0.296036, acc: 0.083080]\n",
      "640  [AE loss: 0.291954, acc: 0.082638]\n",
      "641  [AE loss: 0.289460, acc: 0.083485]\n",
      "642  [AE loss: 0.290413, acc: 0.083457]\n",
      "643  [AE loss: 0.293085, acc: 0.082378]\n",
      "644  [AE loss: 0.292048, acc: 0.083060]\n",
      "645  [AE loss: 0.291167, acc: 0.082462]\n",
      "646  [AE loss: 0.293055, acc: 0.083358]\n",
      "647  [AE loss: 0.301081, acc: 0.082650]\n",
      "648  [AE loss: 0.307443, acc: 0.082777]\n",
      "649  [AE loss: 0.356153, acc: 0.082831]\n",
      "650  [AE loss: 0.296109, acc: 0.083885]\n",
      "651  [AE loss: 0.293691, acc: 0.081813]\n",
      "652  [AE loss: 0.303381, acc: 0.083357]\n",
      "653  [AE loss: 0.297045, acc: 0.082891]\n",
      "654  [AE loss: 0.330839, acc: 0.082235]\n",
      "655  [AE loss: 0.293595, acc: 0.083108]\n",
      "656  [AE loss: 0.291522, acc: 0.083234]\n",
      "657  [AE loss: 0.292466, acc: 0.082754]\n",
      "658  [AE loss: 0.305933, acc: 0.082206]\n",
      "659  [AE loss: 0.308222, acc: 0.082749]\n",
      "660  [AE loss: 0.452314, acc: 0.083185]\n",
      "661  [AE loss: 0.328332, acc: 0.082742]\n",
      "662  [AE loss: 0.301304, acc: 0.081780]\n",
      "663  [AE loss: 0.291941, acc: 0.083335]\n",
      "664  [AE loss: 0.290792, acc: 0.083302]\n",
      "665  [AE loss: 0.291151, acc: 0.081838]\n",
      "666  [AE loss: 0.290499, acc: 0.082786]\n",
      "667  [AE loss: 0.289991, acc: 0.083341]\n",
      "668  [AE loss: 0.295628, acc: 0.082629]\n",
      "669  [AE loss: 0.300199, acc: 0.082600]\n",
      "670  [AE loss: 0.307159, acc: 0.083180]\n",
      "671  [AE loss: 0.410754, acc: 0.082515]\n",
      "672  [AE loss: 0.310479, acc: 0.083276]\n",
      "673  [AE loss: 0.293373, acc: 0.082838]\n",
      "674  [AE loss: 0.292694, acc: 0.082310]\n",
      "675  [AE loss: 0.290746, acc: 0.082261]\n",
      "676  [AE loss: 0.288972, acc: 0.083286]\n",
      "677  [AE loss: 0.290160, acc: 0.082434]\n",
      "678  [AE loss: 0.293148, acc: 0.082394]\n",
      "679  [AE loss: 0.297997, acc: 0.082689]\n",
      "680  [AE loss: 0.323850, acc: 0.082782]\n",
      "681  [AE loss: 0.297616, acc: 0.083284]\n",
      "682  [AE loss: 0.317000, acc: 0.082892]\n",
      "683  [AE loss: 0.308660, acc: 0.082632]\n",
      "684  [AE loss: 0.423232, acc: 0.082600]\n",
      "685  [AE loss: 0.316777, acc: 0.082605]\n",
      "686  [AE loss: 0.293247, acc: 0.082725]\n",
      "687  [AE loss: 0.287662, acc: 0.082813]\n",
      "688  [AE loss: 0.286623, acc: 0.083905]\n",
      "689  [AE loss: 0.293773, acc: 0.083287]\n",
      "690  [AE loss: 0.295182, acc: 0.083241]\n",
      "691  [AE loss: 0.289851, acc: 0.082999]\n",
      "692  [AE loss: 0.300182, acc: 0.082455]\n",
      "693  [AE loss: 0.301740, acc: 0.082888]\n",
      "694  [AE loss: 0.360798, acc: 0.082976]\n",
      "695  [AE loss: 0.294033, acc: 0.082361]\n",
      "696  [AE loss: 0.290982, acc: 0.083771]\n",
      "697  [AE loss: 0.287959, acc: 0.082717]\n",
      "698  [AE loss: 0.293371, acc: 0.083401]\n",
      "699  [AE loss: 0.304174, acc: 0.083256]\n",
      "700  [AE loss: 0.309305, acc: 0.082407]\n",
      "701  [AE loss: 0.434114, acc: 0.081924]\n",
      "702  [AE loss: 0.320861, acc: 0.083140]\n",
      "703  [AE loss: 0.295743, acc: 0.082448]\n",
      "704  [AE loss: 0.291727, acc: 0.081761]\n",
      "705  [AE loss: 0.323126, acc: 0.080294]\n",
      "706  [AE loss: 0.297550, acc: 0.081715]\n",
      "707  [AE loss: 0.298944, acc: 0.082369]\n",
      "708  [AE loss: 0.293067, acc: 0.082788]\n",
      "709  [AE loss: 0.293699, acc: 0.083206]\n",
      "710  [AE loss: 0.290115, acc: 0.083763]\n",
      "711  [AE loss: 0.291494, acc: 0.082617]\n",
      "712  [AE loss: 0.293008, acc: 0.082882]\n",
      "713  [AE loss: 0.299933, acc: 0.082225]\n",
      "714  [AE loss: 0.306407, acc: 0.082849]\n",
      "715  [AE loss: 0.416235, acc: 0.082758]\n",
      "716  [AE loss: 0.307914, acc: 0.082969]\n",
      "717  [AE loss: 0.293597, acc: 0.082917]\n",
      "718  [AE loss: 0.292591, acc: 0.082170]\n",
      "719  [AE loss: 0.291336, acc: 0.082278]\n",
      "720  [AE loss: 0.291855, acc: 0.083563]\n",
      "721  [AE loss: 0.290159, acc: 0.082307]\n",
      "722  [AE loss: 0.290144, acc: 0.082908]\n",
      "723  [AE loss: 0.293447, acc: 0.083508]\n",
      "724  [AE loss: 0.296348, acc: 0.082914]\n",
      "725  [AE loss: 0.319786, acc: 0.082903]\n",
      "726  [AE loss: 0.455584, acc: 0.083142]\n",
      "727  [AE loss: 0.336298, acc: 0.082853]\n",
      "728  [AE loss: 0.300481, acc: 0.083369]\n",
      "729  [AE loss: 0.291806, acc: 0.082471]\n",
      "730  [AE loss: 0.289961, acc: 0.082285]\n",
      "731  [AE loss: 0.291952, acc: 0.083308]\n",
      "732  [AE loss: 0.291649, acc: 0.082936]\n",
      "733  [AE loss: 0.293322, acc: 0.082708]\n",
      "734  [AE loss: 0.292318, acc: 0.082014]\n",
      "735  [AE loss: 0.293320, acc: 0.082438]\n",
      "736  [AE loss: 0.295413, acc: 0.082716]\n",
      "737  [AE loss: 0.293854, acc: 0.083088]\n",
      "738  [AE loss: 0.302604, acc: 0.083361]\n",
      "739  [AE loss: 0.296997, acc: 0.083014]\n",
      "740  [AE loss: 0.325090, acc: 0.082690]\n",
      "741  [AE loss: 0.291538, acc: 0.082885]\n",
      "742  [AE loss: 0.297360, acc: 0.082339]\n",
      "743  [AE loss: 0.298318, acc: 0.082650]\n",
      "744  [AE loss: 0.346019, acc: 0.082874]\n",
      "745  [AE loss: 0.291929, acc: 0.082302]\n",
      "746  [AE loss: 0.298088, acc: 0.082697]\n",
      "747  [AE loss: 0.292963, acc: 0.083125]\n",
      "748  [AE loss: 0.295655, acc: 0.083269]\n",
      "749  [AE loss: 0.309097, acc: 0.082941]\n",
      "750  [AE loss: 0.425768, acc: 0.083091]\n",
      "751  [AE loss: 0.319330, acc: 0.082699]\n",
      "752  [AE loss: 0.295843, acc: 0.082529]\n",
      "753  [AE loss: 0.290864, acc: 0.081526]\n",
      "754  [AE loss: 0.290267, acc: 0.083027]\n",
      "755  [AE loss: 0.293577, acc: 0.082246]\n",
      "756  [AE loss: 0.289777, acc: 0.082241]\n",
      "757  [AE loss: 0.290969, acc: 0.083406]\n",
      "758  [AE loss: 0.290476, acc: 0.083114]\n",
      "759  [AE loss: 0.288188, acc: 0.083254]\n",
      "760  [AE loss: 0.293308, acc: 0.082482]\n",
      "761  [AE loss: 0.303405, acc: 0.083265]\n",
      "762  [AE loss: 0.296866, acc: 0.082463]\n",
      "763  [AE loss: 0.320978, acc: 0.082921]\n",
      "764  [AE loss: 0.300190, acc: 0.082700]\n",
      "765  [AE loss: 0.325918, acc: 0.083038]\n",
      "766  [AE loss: 0.289941, acc: 0.082773]\n",
      "767  [AE loss: 0.291963, acc: 0.082140]\n",
      "768  [AE loss: 0.292016, acc: 0.083094]\n",
      "769  [AE loss: 0.327405, acc: 0.083367]\n",
      "770  [AE loss: 0.292076, acc: 0.083003]\n",
      "771  [AE loss: 0.296880, acc: 0.082889]\n",
      "772  [AE loss: 0.303401, acc: 0.083159]\n",
      "773  [AE loss: 0.385886, acc: 0.082673]\n",
      "774  [AE loss: 0.302308, acc: 0.082754]\n",
      "775  [AE loss: 0.292573, acc: 0.081919]\n",
      "776  [AE loss: 0.292419, acc: 0.083387]\n",
      "777  [AE loss: 0.294396, acc: 0.082423]\n",
      "778  [AE loss: 0.289550, acc: 0.082770]\n",
      "779  [AE loss: 0.291549, acc: 0.082645]\n",
      "780  [AE loss: 0.295152, acc: 0.082534]\n",
      "781  [AE loss: 0.304768, acc: 0.082339]\n",
      "782  [AE loss: 0.350490, acc: 0.082637]\n",
      "783  [AE loss: 0.292560, acc: 0.083868]\n",
      "784  [AE loss: 0.291094, acc: 0.082583]\n",
      "785  [AE loss: 0.291787, acc: 0.082267]\n",
      "786  [AE loss: 0.289016, acc: 0.082767]\n",
      "787  [AE loss: 0.302057, acc: 0.082350]\n",
      "788  [AE loss: 0.314259, acc: 0.082411]\n",
      "789  [AE loss: 0.452522, acc: 0.082788]\n",
      "790  [AE loss: 0.340941, acc: 0.082887]\n",
      "791  [AE loss: 0.310738, acc: 0.083357]\n",
      "792  [AE loss: 0.294962, acc: 0.082418]\n",
      "793  [AE loss: 0.291509, acc: 0.082255]\n",
      "794  [AE loss: 0.288203, acc: 0.082914]\n",
      "795  [AE loss: 0.290471, acc: 0.082277]\n",
      "796  [AE loss: 0.289019, acc: 0.083120]\n",
      "797  [AE loss: 0.292185, acc: 0.083077]\n",
      "798  [AE loss: 0.290266, acc: 0.082532]\n",
      "799  [AE loss: 0.288627, acc: 0.082675]\n",
      "800  [AE loss: 0.291676, acc: 0.083074]\n",
      "801  [AE loss: 0.290554, acc: 0.082018]\n",
      "802  [AE loss: 0.289627, acc: 0.083413]\n",
      "803  [AE loss: 0.299533, acc: 0.082792]\n",
      "804  [AE loss: 0.293693, acc: 0.083414]\n",
      "805  [AE loss: 0.325718, acc: 0.082867]\n",
      "806  [AE loss: 0.291324, acc: 0.082954]\n",
      "807  [AE loss: 0.303648, acc: 0.082542]\n",
      "808  [AE loss: 0.299062, acc: 0.082802]\n",
      "809  [AE loss: 0.340680, acc: 0.083384]\n",
      "810  [AE loss: 0.288409, acc: 0.082450]\n",
      "811  [AE loss: 0.289274, acc: 0.082883]\n",
      "812  [AE loss: 0.289381, acc: 0.083381]\n",
      "813  [AE loss: 0.287474, acc: 0.082991]\n",
      "814  [AE loss: 0.289116, acc: 0.083289]\n",
      "815  [AE loss: 0.288424, acc: 0.083232]\n",
      "816  [AE loss: 0.288215, acc: 0.083027]\n",
      "817  [AE loss: 0.291257, acc: 0.082709]\n",
      "818  [AE loss: 0.312271, acc: 0.082987]\n",
      "819  [AE loss: 0.367458, acc: 0.082715]\n",
      "820  [AE loss: 0.451072, acc: 0.084070]\n",
      "821  [AE loss: 0.366924, acc: 0.082996]\n",
      "822  [AE loss: 0.329894, acc: 0.082701]\n",
      "823  [AE loss: 0.308756, acc: 0.081931]\n",
      "824  [AE loss: 0.299222, acc: 0.082843]\n",
      "825  [AE loss: 0.292919, acc: 0.083180]\n",
      "826  [AE loss: 0.293306, acc: 0.083138]\n",
      "827  [AE loss: 0.288796, acc: 0.082778]\n",
      "828  [AE loss: 0.292148, acc: 0.082539]\n",
      "829  [AE loss: 0.288184, acc: 0.083175]\n",
      "830  [AE loss: 0.291723, acc: 0.082648]\n",
      "831  [AE loss: 0.288534, acc: 0.083168]\n",
      "832  [AE loss: 0.289654, acc: 0.082689]\n",
      "833  [AE loss: 0.287390, acc: 0.082881]\n",
      "834  [AE loss: 0.287536, acc: 0.083157]\n",
      "835  [AE loss: 0.289540, acc: 0.083409]\n",
      "836  [AE loss: 0.291509, acc: 0.082396]\n",
      "837  [AE loss: 0.291148, acc: 0.082979]\n",
      "838  [AE loss: 0.295553, acc: 0.082939]\n",
      "839  [AE loss: 0.294168, acc: 0.082430]\n",
      "840  [AE loss: 0.305233, acc: 0.083672]\n",
      "841  [AE loss: 0.292472, acc: 0.083073]\n",
      "842  [AE loss: 0.311340, acc: 0.082511]\n",
      "843  [AE loss: 0.292665, acc: 0.083075]\n",
      "844  [AE loss: 0.293090, acc: 0.083057]\n",
      "845  [AE loss: 0.297560, acc: 0.082440]\n",
      "846  [AE loss: 0.321995, acc: 0.083147]\n",
      "847  [AE loss: 0.292725, acc: 0.082395]\n",
      "848  [AE loss: 0.293667, acc: 0.082858]\n",
      "849  [AE loss: 0.287868, acc: 0.083029]\n",
      "850  [AE loss: 0.302565, acc: 0.082430]\n",
      "851  [AE loss: 0.304557, acc: 0.083237]\n",
      "852  [AE loss: 0.404153, acc: 0.082911]\n",
      "853  [AE loss: 0.304535, acc: 0.083226]\n",
      "854  [AE loss: 0.293609, acc: 0.082861]\n",
      "855  [AE loss: 0.291949, acc: 0.082803]\n",
      "856  [AE loss: 0.290394, acc: 0.083712]\n",
      "857  [AE loss: 0.290541, acc: 0.082944]\n",
      "858  [AE loss: 0.289758, acc: 0.081731]\n",
      "859  [AE loss: 0.290546, acc: 0.082472]\n",
      "860  [AE loss: 0.289855, acc: 0.082278]\n",
      "861  [AE loss: 0.293017, acc: 0.083000]\n",
      "862  [AE loss: 0.297305, acc: 0.083934]\n",
      "863  [AE loss: 0.308555, acc: 0.083044]\n",
      "864  [AE loss: 0.394965, acc: 0.083101]\n",
      "865  [AE loss: 0.307585, acc: 0.082664]\n",
      "866  [AE loss: 0.291185, acc: 0.082327]\n",
      "867  [AE loss: 0.291008, acc: 0.083032]\n",
      "868  [AE loss: 0.290796, acc: 0.082498]\n",
      "869  [AE loss: 0.292963, acc: 0.083163]\n",
      "870  [AE loss: 0.290975, acc: 0.083429]\n",
      "871  [AE loss: 0.286170, acc: 0.083199]\n",
      "872  [AE loss: 0.286772, acc: 0.083660]\n",
      "873  [AE loss: 0.286468, acc: 0.082921]\n",
      "874  [AE loss: 0.288786, acc: 0.082300]\n",
      "875  [AE loss: 0.289390, acc: 0.083369]\n",
      "876  [AE loss: 0.290709, acc: 0.083575]\n",
      "877  [AE loss: 0.310407, acc: 0.083256]\n",
      "878  [AE loss: 0.334240, acc: 0.082540]\n",
      "879  [AE loss: 0.411794, acc: 0.082352]\n",
      "880  [AE loss: 0.317786, acc: 0.083019]\n",
      "881  [AE loss: 0.299245, acc: 0.082742]\n",
      "882  [AE loss: 0.292248, acc: 0.082531]\n",
      "883  [AE loss: 0.290664, acc: 0.083115]\n",
      "884  [AE loss: 0.289803, acc: 0.082906]\n",
      "885  [AE loss: 0.290364, acc: 0.082261]\n",
      "886  [AE loss: 0.291975, acc: 0.082473]\n",
      "887  [AE loss: 0.288130, acc: 0.082274]\n",
      "888  [AE loss: 0.286945, acc: 0.083483]\n",
      "889  [AE loss: 0.290208, acc: 0.082903]\n",
      "890  [AE loss: 0.286819, acc: 0.083206]\n",
      "891  [AE loss: 0.290789, acc: 0.083593]\n",
      "892  [AE loss: 0.297357, acc: 0.083073]\n",
      "893  [AE loss: 0.301848, acc: 0.082634]\n",
      "894  [AE loss: 0.376260, acc: 0.082817]\n",
      "895  [AE loss: 0.295483, acc: 0.082880]\n",
      "896  [AE loss: 0.289358, acc: 0.082437]\n",
      "897  [AE loss: 0.290227, acc: 0.083151]\n",
      "898  [AE loss: 0.290165, acc: 0.083518]\n",
      "899  [AE loss: 0.291650, acc: 0.083511]\n",
      "900  [AE loss: 0.293521, acc: 0.082227]\n",
      "901  [AE loss: 0.301632, acc: 0.081832]\n",
      "902  [AE loss: 0.295860, acc: 0.082117]\n",
      "903  [AE loss: 0.307898, acc: 0.082772]\n",
      "904  [AE loss: 0.293383, acc: 0.082598]\n",
      "905  [AE loss: 0.306252, acc: 0.083094]\n",
      "906  [AE loss: 0.292587, acc: 0.082826]\n",
      "907  [AE loss: 0.301275, acc: 0.082688]\n",
      "908  [AE loss: 0.292003, acc: 0.082784]\n",
      "909  [AE loss: 0.297137, acc: 0.082632]\n",
      "910  [AE loss: 0.297052, acc: 0.082559]\n",
      "911  [AE loss: 0.324528, acc: 0.082993]\n",
      "912  [AE loss: 0.290137, acc: 0.082982]\n",
      "913  [AE loss: 0.293380, acc: 0.083351]\n",
      "914  [AE loss: 0.293076, acc: 0.083069]\n",
      "915  [AE loss: 0.298154, acc: 0.082578]\n",
      "916  [AE loss: 0.296971, acc: 0.082279]\n",
      "917  [AE loss: 0.336859, acc: 0.083213]\n",
      "918  [AE loss: 0.294550, acc: 0.083478]\n",
      "919  [AE loss: 0.289651, acc: 0.082080]\n",
      "920  [AE loss: 0.289120, acc: 0.083289]\n",
      "921  [AE loss: 0.290417, acc: 0.083082]\n",
      "922  [AE loss: 0.291312, acc: 0.082721]\n",
      "923  [AE loss: 0.292263, acc: 0.082533]\n",
      "924  [AE loss: 0.303185, acc: 0.082267]\n",
      "925  [AE loss: 0.358778, acc: 0.083395]\n",
      "926  [AE loss: 0.290857, acc: 0.081975]\n",
      "927  [AE loss: 0.290422, acc: 0.082104]\n",
      "928  [AE loss: 0.292316, acc: 0.082047]\n",
      "929  [AE loss: 0.290551, acc: 0.083049]\n",
      "930  [AE loss: 0.291661, acc: 0.082267]\n",
      "931  [AE loss: 0.301401, acc: 0.082535]\n",
      "932  [AE loss: 0.300129, acc: 0.082977]\n",
      "933  [AE loss: 0.345851, acc: 0.082543]\n",
      "934  [AE loss: 0.298812, acc: 0.083000]\n",
      "935  [AE loss: 0.295156, acc: 0.082609]\n",
      "936  [AE loss: 0.292532, acc: 0.082352]\n",
      "937  [AE loss: 0.291748, acc: 0.082598]\n",
      "938  [AE loss: 0.289930, acc: 0.083461]\n",
      "939  [AE loss: 0.290968, acc: 0.083051]\n",
      "940  [AE loss: 0.290327, acc: 0.082786]\n",
      "941  [AE loss: 0.294773, acc: 0.082651]\n",
      "942  [AE loss: 0.380824, acc: 0.082605]\n",
      "943  [AE loss: 0.292357, acc: 0.082972]\n",
      "944  [AE loss: 0.291616, acc: 0.082317]\n",
      "945  [AE loss: 0.293379, acc: 0.082513]\n",
      "946  [AE loss: 0.292261, acc: 0.082506]\n",
      "947  [AE loss: 0.290606, acc: 0.083159]\n",
      "948  [AE loss: 0.288668, acc: 0.083052]\n",
      "949  [AE loss: 0.291416, acc: 0.082744]\n",
      "950  [AE loss: 0.291257, acc: 0.082760]\n",
      "951  [AE loss: 0.292918, acc: 0.082722]\n",
      "952  [AE loss: 0.296005, acc: 0.083408]\n",
      "953  [AE loss: 0.354624, acc: 0.081975]\n",
      "954  [AE loss: 0.293461, acc: 0.082644]\n",
      "955  [AE loss: 0.291685, acc: 0.083020]\n",
      "956  [AE loss: 0.289304, acc: 0.082393]\n",
      "957  [AE loss: 0.287973, acc: 0.082581]\n",
      "958  [AE loss: 0.290221, acc: 0.083010]\n",
      "959  [AE loss: 0.294028, acc: 0.082504]\n",
      "960  [AE loss: 0.290601, acc: 0.083666]\n",
      "961  [AE loss: 0.300842, acc: 0.083663]\n",
      "962  [AE loss: 0.308489, acc: 0.082780]\n",
      "963  [AE loss: 0.384601, acc: 0.082643]\n",
      "964  [AE loss: 0.303671, acc: 0.081597]\n",
      "965  [AE loss: 0.291159, acc: 0.082638]\n",
      "966  [AE loss: 0.288055, acc: 0.083115]\n",
      "967  [AE loss: 0.289204, acc: 0.083361]\n",
      "968  [AE loss: 0.290307, acc: 0.082413]\n",
      "969  [AE loss: 0.289535, acc: 0.082554]\n",
      "970  [AE loss: 0.288418, acc: 0.082906]\n",
      "971  [AE loss: 0.289392, acc: 0.083267]\n",
      "972  [AE loss: 0.288933, acc: 0.082629]\n",
      "973  [AE loss: 0.290344, acc: 0.083175]\n",
      "974  [AE loss: 0.290977, acc: 0.083595]\n",
      "975  [AE loss: 0.297626, acc: 0.083193]\n",
      "976  [AE loss: 0.297477, acc: 0.083439]\n",
      "977  [AE loss: 0.349501, acc: 0.082725]\n",
      "978  [AE loss: 0.291805, acc: 0.083458]\n",
      "979  [AE loss: 0.292869, acc: 0.082106]\n",
      "980  [AE loss: 0.291487, acc: 0.083921]\n",
      "981  [AE loss: 0.288104, acc: 0.082626]\n",
      "982  [AE loss: 0.290418, acc: 0.082252]\n",
      "983  [AE loss: 0.292931, acc: 0.083368]\n",
      "984  [AE loss: 0.287501, acc: 0.082263]\n",
      "985  [AE loss: 0.288828, acc: 0.083276]\n",
      "986  [AE loss: 0.290014, acc: 0.082814]\n",
      "987  [AE loss: 0.296760, acc: 0.082926]\n",
      "988  [AE loss: 0.327907, acc: 0.082556]\n",
      "989  [AE loss: 0.292628, acc: 0.083209]\n",
      "990  [AE loss: 0.297256, acc: 0.082622]\n",
      "991  [AE loss: 0.293906, acc: 0.082933]\n",
      "992  [AE loss: 0.319756, acc: 0.082874]\n",
      "993  [AE loss: 0.291591, acc: 0.083168]\n",
      "994  [AE loss: 0.295283, acc: 0.082896]\n",
      "995  [AE loss: 0.290862, acc: 0.082462]\n",
      "996  [AE loss: 0.289684, acc: 0.083264]\n",
      "997  [AE loss: 0.290545, acc: 0.082374]\n",
      "998  [AE loss: 0.308791, acc: 0.082736]\n",
      "999  [AE loss: 0.294763, acc: 0.082173]\n",
      "1000  [AE loss: 0.302665, acc: 0.082046]\n",
      "1001  [AE loss: 0.296174, acc: 0.083108]\n",
      "1002  [AE loss: 0.320955, acc: 0.082253]\n",
      "1003  [AE loss: 0.293809, acc: 0.084103]\n",
      "1004  [AE loss: 0.292294, acc: 0.083002]\n",
      "1005  [AE loss: 0.290479, acc: 0.083552]\n",
      "1006  [AE loss: 0.295784, acc: 0.082697]\n",
      "1007  [AE loss: 0.288756, acc: 0.082706]\n",
      "1008  [AE loss: 0.294639, acc: 0.083870]\n",
      "1009  [AE loss: 0.293108, acc: 0.083425]\n",
      "1010  [AE loss: 0.300645, acc: 0.083278]\n",
      "1011  [AE loss: 0.295872, acc: 0.083298]\n",
      "1012  [AE loss: 0.336345, acc: 0.083580]\n",
      "1013  [AE loss: 0.292553, acc: 0.082676]\n",
      "1014  [AE loss: 0.294053, acc: 0.082136]\n",
      "1015  [AE loss: 0.296766, acc: 0.083300]\n",
      "1016  [AE loss: 0.291896, acc: 0.082617]\n",
      "1017  [AE loss: 0.296277, acc: 0.083059]\n",
      "1018  [AE loss: 0.295995, acc: 0.083341]\n",
      "1019  [AE loss: 0.302394, acc: 0.083378]\n",
      "1020  [AE loss: 0.292886, acc: 0.082385]\n",
      "1021  [AE loss: 0.293220, acc: 0.082168]\n",
      "1022  [AE loss: 0.293941, acc: 0.082930]\n",
      "1023  [AE loss: 0.295597, acc: 0.082672]\n",
      "1024  [AE loss: 0.290289, acc: 0.082626]\n",
      "1025  [AE loss: 0.321218, acc: 0.082513]\n",
      "1026  [AE loss: 0.297768, acc: 0.082638]\n",
      "1027  [AE loss: 0.309000, acc: 0.083026]\n",
      "1028  [AE loss: 0.287014, acc: 0.081958]\n",
      "1029  [AE loss: 0.290760, acc: 0.083004]\n",
      "1030  [AE loss: 0.289355, acc: 0.083331]\n",
      "1031  [AE loss: 0.293356, acc: 0.082665]\n",
      "1032  [AE loss: 0.297559, acc: 0.082551]\n",
      "1033  [AE loss: 0.337793, acc: 0.083662]\n",
      "1034  [AE loss: 0.290979, acc: 0.083271]\n",
      "1035  [AE loss: 0.301314, acc: 0.082434]\n",
      "1036  [AE loss: 0.319641, acc: 0.082068]\n",
      "1037  [AE loss: 0.295712, acc: 0.082457]\n",
      "1038  [AE loss: 0.292129, acc: 0.082645]\n",
      "1039  [AE loss: 0.290214, acc: 0.083508]\n",
      "1040  [AE loss: 0.291908, acc: 0.082526]\n",
      "1041  [AE loss: 0.295689, acc: 0.082786]\n",
      "1042  [AE loss: 0.290109, acc: 0.082364]\n",
      "1043  [AE loss: 0.297219, acc: 0.082738]\n",
      "1044  [AE loss: 0.288207, acc: 0.082379]\n",
      "1045  [AE loss: 0.292380, acc: 0.083260]\n",
      "1046  [AE loss: 0.297660, acc: 0.083917]\n",
      "1047  [AE loss: 0.325372, acc: 0.083218]\n",
      "1048  [AE loss: 0.289020, acc: 0.082461]\n",
      "1049  [AE loss: 0.290862, acc: 0.082230]\n",
      "1050  [AE loss: 0.291333, acc: 0.082666]\n",
      "1051  [AE loss: 0.291829, acc: 0.082299]\n",
      "1052  [AE loss: 0.294976, acc: 0.082961]\n",
      "1053  [AE loss: 0.294570, acc: 0.082991]\n",
      "1054  [AE loss: 0.303774, acc: 0.083452]\n",
      "1055  [AE loss: 0.287756, acc: 0.082867]\n",
      "1056  [AE loss: 0.296847, acc: 0.083446]\n",
      "1057  [AE loss: 0.290578, acc: 0.082850]\n",
      "1058  [AE loss: 0.298026, acc: 0.082268]\n",
      "1059  [AE loss: 0.291514, acc: 0.082721]\n",
      "1060  [AE loss: 0.296217, acc: 0.083129]\n",
      "1061  [AE loss: 0.293061, acc: 0.082489]\n",
      "1062  [AE loss: 0.297152, acc: 0.082584]\n",
      "1063  [AE loss: 0.292037, acc: 0.082667]\n",
      "1064  [AE loss: 0.291939, acc: 0.082322]\n",
      "1065  [AE loss: 0.292822, acc: 0.082690]\n",
      "1066  [AE loss: 0.305656, acc: 0.082744]\n",
      "1067  [AE loss: 0.289310, acc: 0.082159]\n",
      "1068  [AE loss: 0.292374, acc: 0.082357]\n",
      "1069  [AE loss: 0.294224, acc: 0.082604]\n",
      "1070  [AE loss: 0.319993, acc: 0.083331]\n",
      "1071  [AE loss: 0.289300, acc: 0.083053]\n",
      "1072  [AE loss: 0.289535, acc: 0.082388]\n",
      "1073  [AE loss: 0.289041, acc: 0.083340]\n",
      "1074  [AE loss: 0.285766, acc: 0.082577]\n",
      "1075  [AE loss: 0.288925, acc: 0.082876]\n",
      "1076  [AE loss: 0.297800, acc: 0.082676]\n",
      "1077  [AE loss: 0.296566, acc: 0.083340]\n",
      "1078  [AE loss: 0.333555, acc: 0.082933]\n",
      "1079  [AE loss: 0.293566, acc: 0.082935]\n",
      "1080  [AE loss: 0.294244, acc: 0.082902]\n",
      "1081  [AE loss: 0.293244, acc: 0.083579]\n",
      "1082  [AE loss: 0.291703, acc: 0.082542]\n",
      "1083  [AE loss: 0.289933, acc: 0.082853]\n",
      "1084  [AE loss: 0.290978, acc: 0.082855]\n",
      "1085  [AE loss: 0.293151, acc: 0.082964]\n",
      "1086  [AE loss: 0.289405, acc: 0.083306]\n",
      "1087  [AE loss: 0.299279, acc: 0.082880]\n",
      "1088  [AE loss: 0.293372, acc: 0.083542]\n",
      "1089  [AE loss: 0.302068, acc: 0.082762]\n",
      "1090  [AE loss: 0.293664, acc: 0.083123]\n",
      "1091  [AE loss: 0.292706, acc: 0.083241]\n",
      "1092  [AE loss: 0.290118, acc: 0.082385]\n",
      "1093  [AE loss: 0.296924, acc: 0.083230]\n",
      "1094  [AE loss: 0.291257, acc: 0.083983]\n",
      "1095  [AE loss: 0.299520, acc: 0.082285]\n",
      "1096  [AE loss: 0.292144, acc: 0.082197]\n",
      "1097  [AE loss: 0.318859, acc: 0.082998]\n",
      "1098  [AE loss: 0.294603, acc: 0.083440]\n",
      "1099  [AE loss: 0.289608, acc: 0.083398]\n",
      "1100  [AE loss: 0.288967, acc: 0.082836]\n",
      "1101  [AE loss: 0.295156, acc: 0.082908]\n",
      "1102  [AE loss: 0.292504, acc: 0.083163]\n",
      "1103  [AE loss: 0.299174, acc: 0.082783]\n",
      "1104  [AE loss: 0.293419, acc: 0.083571]\n",
      "1105  [AE loss: 0.294021, acc: 0.082344]\n",
      "1106  [AE loss: 0.290110, acc: 0.082310]\n",
      "1107  [AE loss: 0.296092, acc: 0.082007]\n",
      "1108  [AE loss: 0.290760, acc: 0.082853]\n",
      "1109  [AE loss: 0.308691, acc: 0.082798]\n",
      "1110  [AE loss: 0.291894, acc: 0.082430]\n",
      "1111  [AE loss: 0.291604, acc: 0.082102]\n",
      "1112  [AE loss: 0.293237, acc: 0.081443]\n",
      "1113  [AE loss: 0.287002, acc: 0.082731]\n",
      "1114  [AE loss: 0.287599, acc: 0.083008]\n",
      "1115  [AE loss: 0.291348, acc: 0.082771]\n",
      "1116  [AE loss: 0.295873, acc: 0.083053]\n",
      "1117  [AE loss: 0.307735, acc: 0.083071]\n",
      "1118  [AE loss: 0.345796, acc: 0.082079]\n",
      "1119  [AE loss: 0.295456, acc: 0.082640]\n",
      "1120  [AE loss: 0.291909, acc: 0.083009]\n",
      "1121  [AE loss: 0.298369, acc: 0.082535]\n",
      "1122  [AE loss: 0.289541, acc: 0.081738]\n",
      "1123  [AE loss: 0.288234, acc: 0.082958]\n",
      "1124  [AE loss: 0.289028, acc: 0.082778]\n",
      "1125  [AE loss: 0.290039, acc: 0.083787]\n",
      "1126  [AE loss: 0.294280, acc: 0.082549]\n",
      "1127  [AE loss: 0.291812, acc: 0.083074]\n",
      "1128  [AE loss: 0.340391, acc: 0.083379]\n",
      "1129  [AE loss: 0.287608, acc: 0.083077]\n",
      "1130  [AE loss: 0.291736, acc: 0.082617]\n",
      "1131  [AE loss: 0.288296, acc: 0.082968]\n",
      "1132  [AE loss: 0.289003, acc: 0.081958]\n",
      "1133  [AE loss: 0.288123, acc: 0.083320]\n",
      "1134  [AE loss: 0.287715, acc: 0.082518]\n",
      "1135  [AE loss: 0.287518, acc: 0.082996]\n",
      "1136  [AE loss: 0.290578, acc: 0.083390]\n",
      "1137  [AE loss: 0.289250, acc: 0.083140]\n",
      "1138  [AE loss: 0.289036, acc: 0.083070]\n",
      "1139  [AE loss: 0.292649, acc: 0.082347]\n",
      "1140  [AE loss: 0.295664, acc: 0.083138]\n",
      "1141  [AE loss: 0.316922, acc: 0.082983]\n",
      "1142  [AE loss: 0.294724, acc: 0.083241]\n",
      "1143  [AE loss: 0.298569, acc: 0.082894]\n",
      "1144  [AE loss: 0.291018, acc: 0.082568]\n",
      "1145  [AE loss: 0.291930, acc: 0.084551]\n",
      "1146  [AE loss: 0.287549, acc: 0.082584]\n",
      "1147  [AE loss: 0.299205, acc: 0.083264]\n",
      "1148  [AE loss: 0.291701, acc: 0.082908]\n",
      "1149  [AE loss: 0.295110, acc: 0.082875]\n",
      "1150  [AE loss: 0.290561, acc: 0.082836]\n",
      "1151  [AE loss: 0.302442, acc: 0.082897]\n",
      "1152  [AE loss: 0.293548, acc: 0.083102]\n",
      "1153  [AE loss: 0.300987, acc: 0.082570]\n",
      "1154  [AE loss: 0.288204, acc: 0.083192]\n",
      "1155  [AE loss: 0.293461, acc: 0.082676]\n",
      "1156  [AE loss: 0.291476, acc: 0.082894]\n",
      "1157  [AE loss: 0.288912, acc: 0.082936]\n",
      "1158  [AE loss: 0.286777, acc: 0.082612]\n",
      "1159  [AE loss: 0.295253, acc: 0.082728]\n",
      "1160  [AE loss: 0.298054, acc: 0.082948]\n",
      "1161  [AE loss: 0.327434, acc: 0.082241]\n",
      "1162  [AE loss: 0.289977, acc: 0.082308]\n",
      "1163  [AE loss: 0.296160, acc: 0.083053]\n",
      "1164  [AE loss: 0.298087, acc: 0.082231]\n",
      "1165  [AE loss: 0.299814, acc: 0.082219]\n",
      "1166  [AE loss: 0.330586, acc: 0.083214]\n",
      "1167  [AE loss: 0.288144, acc: 0.083539]\n",
      "1168  [AE loss: 0.290803, acc: 0.083788]\n",
      "1169  [AE loss: 0.286649, acc: 0.082751]\n",
      "1170  [AE loss: 0.285925, acc: 0.082902]\n",
      "1171  [AE loss: 0.290914, acc: 0.082969]\n",
      "1172  [AE loss: 0.285998, acc: 0.082598]\n",
      "1173  [AE loss: 0.286884, acc: 0.083131]\n",
      "1174  [AE loss: 0.292913, acc: 0.082999]\n",
      "1175  [AE loss: 0.293619, acc: 0.082803]\n",
      "1176  [AE loss: 0.309898, acc: 0.083434]\n",
      "1177  [AE loss: 0.291787, acc: 0.083077]\n",
      "1178  [AE loss: 0.289082, acc: 0.083271]\n",
      "1179  [AE loss: 0.293120, acc: 0.082693]\n",
      "1180  [AE loss: 0.302192, acc: 0.083610]\n",
      "1181  [AE loss: 0.291018, acc: 0.082664]\n",
      "1182  [AE loss: 0.299879, acc: 0.083392]\n",
      "1183  [AE loss: 0.297748, acc: 0.083192]\n",
      "1184  [AE loss: 0.301670, acc: 0.083295]\n",
      "1185  [AE loss: 0.300300, acc: 0.082883]\n",
      "1186  [AE loss: 0.323630, acc: 0.083462]\n",
      "1187  [AE loss: 0.290575, acc: 0.082827]\n",
      "1188  [AE loss: 0.288643, acc: 0.083491]\n",
      "1189  [AE loss: 0.292421, acc: 0.082705]\n",
      "1190  [AE loss: 0.290379, acc: 0.082725]\n",
      "1191  [AE loss: 0.288694, acc: 0.083126]\n",
      "1192  [AE loss: 0.290460, acc: 0.083751]\n",
      "1193  [AE loss: 0.290425, acc: 0.083347]\n",
      "1194  [AE loss: 0.290328, acc: 0.083059]\n",
      "1195  [AE loss: 0.295817, acc: 0.083044]\n",
      "1196  [AE loss: 0.297628, acc: 0.083131]\n",
      "1197  [AE loss: 0.309595, acc: 0.082345]\n",
      "1198  [AE loss: 0.287312, acc: 0.083307]\n",
      "1199  [AE loss: 0.290267, acc: 0.083005]\n",
      "1200  [AE loss: 0.288719, acc: 0.082141]\n",
      "1201  [AE loss: 0.287909, acc: 0.082606]\n",
      "1202  [AE loss: 0.292494, acc: 0.083458]\n",
      "1203  [AE loss: 0.304953, acc: 0.082985]\n",
      "1204  [AE loss: 0.290617, acc: 0.083363]\n",
      "1205  [AE loss: 0.286309, acc: 0.082589]\n",
      "1206  [AE loss: 0.288728, acc: 0.083010]\n",
      "1207  [AE loss: 0.289357, acc: 0.082662]\n",
      "1208  [AE loss: 0.284941, acc: 0.083011]\n",
      "1209  [AE loss: 0.291131, acc: 0.083258]\n",
      "1210  [AE loss: 0.307484, acc: 0.082989]\n",
      "1211  [AE loss: 0.350084, acc: 0.083130]\n",
      "1212  [AE loss: 0.296245, acc: 0.082592]\n",
      "1213  [AE loss: 0.289981, acc: 0.082642]\n",
      "1214  [AE loss: 0.288983, acc: 0.082424]\n",
      "1215  [AE loss: 0.290126, acc: 0.083387]\n",
      "1216  [AE loss: 0.288684, acc: 0.082952]\n",
      "1217  [AE loss: 0.289730, acc: 0.082876]\n",
      "1218  [AE loss: 0.290474, acc: 0.082729]\n",
      "1219  [AE loss: 0.287319, acc: 0.083918]\n",
      "1220  [AE loss: 0.285767, acc: 0.082935]\n",
      "1221  [AE loss: 0.286986, acc: 0.082262]\n",
      "1222  [AE loss: 0.286657, acc: 0.082753]\n",
      "1223  [AE loss: 0.285946, acc: 0.082686]\n",
      "1224  [AE loss: 0.296485, acc: 0.082952]\n",
      "1225  [AE loss: 0.298553, acc: 0.082649]\n",
      "1226  [AE loss: 0.337146, acc: 0.083516]\n",
      "1227  [AE loss: 0.290995, acc: 0.083191]\n",
      "1228  [AE loss: 0.289354, acc: 0.082091]\n",
      "1229  [AE loss: 0.287052, acc: 0.083391]\n",
      "1230  [AE loss: 0.285954, acc: 0.083031]\n",
      "1231  [AE loss: 0.287733, acc: 0.083279]\n",
      "1232  [AE loss: 0.287871, acc: 0.081923]\n",
      "1233  [AE loss: 0.288947, acc: 0.083545]\n",
      "1234  [AE loss: 0.286975, acc: 0.081688]\n",
      "1235  [AE loss: 0.289668, acc: 0.082913]\n",
      "1236  [AE loss: 0.292892, acc: 0.083888]\n",
      "1237  [AE loss: 0.292024, acc: 0.083143]\n",
      "1238  [AE loss: 0.313234, acc: 0.082457]\n",
      "1239  [AE loss: 0.292027, acc: 0.082034]\n",
      "1240  [AE loss: 0.293552, acc: 0.082969]\n",
      "1241  [AE loss: 0.293032, acc: 0.083398]\n",
      "1242  [AE loss: 0.295856, acc: 0.082751]\n",
      "1243  [AE loss: 0.288889, acc: 0.082958]\n",
      "1244  [AE loss: 0.285884, acc: 0.082490]\n",
      "1245  [AE loss: 0.289358, acc: 0.083142]\n",
      "1246  [AE loss: 0.286273, acc: 0.082517]\n",
      "1247  [AE loss: 0.292689, acc: 0.082394]\n",
      "1248  [AE loss: 0.308087, acc: 0.083315]\n",
      "1249  [AE loss: 0.298731, acc: 0.082897]\n",
      "1250  [AE loss: 0.314310, acc: 0.082197]\n",
      "1251  [AE loss: 0.290055, acc: 0.082223]\n",
      "1252  [AE loss: 0.288674, acc: 0.082618]\n",
      "1253  [AE loss: 0.290245, acc: 0.082319]\n",
      "1254  [AE loss: 0.288136, acc: 0.082583]\n",
      "1255  [AE loss: 0.288606, acc: 0.082467]\n",
      "1256  [AE loss: 0.288017, acc: 0.082975]\n",
      "1257  [AE loss: 0.286278, acc: 0.083462]\n",
      "1258  [AE loss: 0.290210, acc: 0.082239]\n",
      "1259  [AE loss: 0.293037, acc: 0.083685]\n",
      "1260  [AE loss: 0.290496, acc: 0.082301]\n",
      "1261  [AE loss: 0.292974, acc: 0.083387]\n",
      "1262  [AE loss: 0.286941, acc: 0.083219]\n",
      "1263  [AE loss: 0.290941, acc: 0.083217]\n",
      "1264  [AE loss: 0.302734, acc: 0.083145]\n",
      "1265  [AE loss: 0.330972, acc: 0.083197]\n",
      "1266  [AE loss: 0.300641, acc: 0.082363]\n",
      "1267  [AE loss: 0.293760, acc: 0.083334]\n",
      "1268  [AE loss: 0.288985, acc: 0.082842]\n",
      "1269  [AE loss: 0.287890, acc: 0.082242]\n",
      "1270  [AE loss: 0.289217, acc: 0.082839]\n",
      "1271  [AE loss: 0.285713, acc: 0.083353]\n",
      "1272  [AE loss: 0.286718, acc: 0.083135]\n",
      "1273  [AE loss: 0.286897, acc: 0.082659]\n",
      "1274  [AE loss: 0.287917, acc: 0.082974]\n",
      "1275  [AE loss: 0.288129, acc: 0.082546]\n",
      "1276  [AE loss: 0.297911, acc: 0.082760]\n",
      "1277  [AE loss: 0.290687, acc: 0.084309]\n",
      "1278  [AE loss: 0.298889, acc: 0.083627]\n",
      "1279  [AE loss: 0.291556, acc: 0.082126]\n",
      "1280  [AE loss: 0.291430, acc: 0.082192]\n",
      "1281  [AE loss: 0.288064, acc: 0.082913]\n",
      "1282  [AE loss: 0.285744, acc: 0.082766]\n",
      "1283  [AE loss: 0.291809, acc: 0.083256]\n",
      "1284  [AE loss: 0.301433, acc: 0.082751]\n",
      "1285  [AE loss: 0.298195, acc: 0.082762]\n",
      "1286  [AE loss: 0.307393, acc: 0.083855]\n",
      "1287  [AE loss: 0.289865, acc: 0.082102]\n",
      "1288  [AE loss: 0.291022, acc: 0.083641]\n",
      "1289  [AE loss: 0.289712, acc: 0.082272]\n",
      "1290  [AE loss: 0.287514, acc: 0.082736]\n",
      "1291  [AE loss: 0.287565, acc: 0.083130]\n",
      "1292  [AE loss: 0.288885, acc: 0.082830]\n",
      "1293  [AE loss: 0.285098, acc: 0.082147]\n",
      "1294  [AE loss: 0.288879, acc: 0.082843]\n",
      "1295  [AE loss: 0.287681, acc: 0.082360]\n",
      "1296  [AE loss: 0.289595, acc: 0.082942]\n",
      "1297  [AE loss: 0.294819, acc: 0.083608]\n",
      "1298  [AE loss: 0.291077, acc: 0.082675]\n",
      "1299  [AE loss: 0.296765, acc: 0.082587]\n",
      "1300  [AE loss: 0.291854, acc: 0.083457]\n",
      "1301  [AE loss: 0.300568, acc: 0.082358]\n",
      "1302  [AE loss: 0.306457, acc: 0.082667]\n",
      "1303  [AE loss: 0.336435, acc: 0.083136]\n",
      "1304  [AE loss: 0.383301, acc: 0.082307]\n",
      "1305  [AE loss: 0.294322, acc: 0.083345]\n",
      "1306  [AE loss: 0.291719, acc: 0.081482]\n",
      "1307  [AE loss: 0.290253, acc: 0.082793]\n",
      "1308  [AE loss: 0.290214, acc: 0.082838]\n",
      "1309  [AE loss: 0.288779, acc: 0.082321]\n",
      "1310  [AE loss: 0.287634, acc: 0.082886]\n",
      "1311  [AE loss: 0.285838, acc: 0.082485]\n",
      "1312  [AE loss: 0.286730, acc: 0.082539]\n",
      "1313  [AE loss: 0.287155, acc: 0.082797]\n",
      "1314  [AE loss: 0.289637, acc: 0.083523]\n",
      "1315  [AE loss: 0.288753, acc: 0.082091]\n",
      "1316  [AE loss: 0.292476, acc: 0.082450]\n",
      "1317  [AE loss: 0.291910, acc: 0.083512]\n",
      "1318  [AE loss: 0.296243, acc: 0.082662]\n",
      "1319  [AE loss: 0.288691, acc: 0.083301]\n",
      "1320  [AE loss: 0.289138, acc: 0.082910]\n",
      "1321  [AE loss: 0.287793, acc: 0.081896]\n",
      "1322  [AE loss: 0.288463, acc: 0.082773]\n",
      "1323  [AE loss: 0.289952, acc: 0.083441]\n",
      "1324  [AE loss: 0.296366, acc: 0.082776]\n",
      "1325  [AE loss: 0.292365, acc: 0.082814]\n",
      "1326  [AE loss: 0.288683, acc: 0.083374]\n",
      "1327  [AE loss: 0.286845, acc: 0.082860]\n",
      "1328  [AE loss: 0.293886, acc: 0.082819]\n",
      "1329  [AE loss: 0.295819, acc: 0.082554]\n",
      "1330  [AE loss: 0.300514, acc: 0.081743]\n",
      "1331  [AE loss: 0.287450, acc: 0.083472]\n",
      "1332  [AE loss: 0.289576, acc: 0.082537]\n",
      "1333  [AE loss: 0.288201, acc: 0.082635]\n",
      "1334  [AE loss: 0.288082, acc: 0.083527]\n",
      "1335  [AE loss: 0.288277, acc: 0.082495]\n",
      "1336  [AE loss: 0.288203, acc: 0.083696]\n",
      "1337  [AE loss: 0.285157, acc: 0.083777]\n",
      "1338  [AE loss: 0.286193, acc: 0.082118]\n",
      "1339  [AE loss: 0.287261, acc: 0.082169]\n",
      "1340  [AE loss: 0.294243, acc: 0.082448]\n",
      "1341  [AE loss: 0.300874, acc: 0.082335]\n",
      "1342  [AE loss: 0.292037, acc: 0.083247]\n",
      "1343  [AE loss: 0.295867, acc: 0.082985]\n",
      "1344  [AE loss: 0.287636, acc: 0.083192]\n",
      "1345  [AE loss: 0.289659, acc: 0.082719]\n",
      "1346  [AE loss: 0.294151, acc: 0.082777]\n",
      "1347  [AE loss: 0.296307, acc: 0.083387]\n",
      "1348  [AE loss: 0.291840, acc: 0.083146]\n",
      "1349  [AE loss: 0.290737, acc: 0.083032]\n",
      "1350  [AE loss: 0.287063, acc: 0.082587]\n",
      "1351  [AE loss: 0.285563, acc: 0.082490]\n",
      "1352  [AE loss: 0.286606, acc: 0.082769]\n",
      "1353  [AE loss: 0.287248, acc: 0.083070]\n",
      "1354  [AE loss: 0.295025, acc: 0.082554]\n",
      "1355  [AE loss: 0.313610, acc: 0.082773]\n",
      "1356  [AE loss: 0.292239, acc: 0.082963]\n",
      "1357  [AE loss: 0.291408, acc: 0.082950]\n",
      "1358  [AE loss: 0.287135, acc: 0.082419]\n",
      "1359  [AE loss: 0.286264, acc: 0.082661]\n",
      "1360  [AE loss: 0.287780, acc: 0.083680]\n",
      "1361  [AE loss: 0.288752, acc: 0.082854]\n",
      "1362  [AE loss: 0.287973, acc: 0.082328]\n",
      "1363  [AE loss: 0.288488, acc: 0.082300]\n",
      "1364  [AE loss: 0.296797, acc: 0.082971]\n",
      "1365  [AE loss: 0.290075, acc: 0.083077]\n",
      "1366  [AE loss: 0.296189, acc: 0.082897]\n",
      "1367  [AE loss: 0.290276, acc: 0.083246]\n",
      "1368  [AE loss: 0.293529, acc: 0.082842]\n",
      "1369  [AE loss: 0.291043, acc: 0.083428]\n",
      "1370  [AE loss: 0.296620, acc: 0.083036]\n",
      "1371  [AE loss: 0.290521, acc: 0.083066]\n",
      "1372  [AE loss: 0.293923, acc: 0.083313]\n",
      "1373  [AE loss: 0.288153, acc: 0.084026]\n",
      "1374  [AE loss: 0.287927, acc: 0.082599]\n",
      "1375  [AE loss: 0.288240, acc: 0.082899]\n",
      "1376  [AE loss: 0.286082, acc: 0.083853]\n",
      "1377  [AE loss: 0.285357, acc: 0.082771]\n",
      "1378  [AE loss: 0.286723, acc: 0.083398]\n",
      "1379  [AE loss: 0.287548, acc: 0.083201]\n",
      "1380  [AE loss: 0.285494, acc: 0.082842]\n",
      "1381  [AE loss: 0.286848, acc: 0.083353]\n",
      "1382  [AE loss: 0.302449, acc: 0.083363]\n",
      "1383  [AE loss: 0.294850, acc: 0.082799]\n",
      "1384  [AE loss: 0.319867, acc: 0.083752]\n",
      "1385  [AE loss: 0.329257, acc: 0.082766]\n",
      "1386  [AE loss: 0.303135, acc: 0.082474]\n",
      "1387  [AE loss: 0.292486, acc: 0.082488]\n",
      "1388  [AE loss: 0.294094, acc: 0.082843]\n",
      "1389  [AE loss: 0.287545, acc: 0.082942]\n",
      "1390  [AE loss: 0.286941, acc: 0.082926]\n",
      "1391  [AE loss: 0.284198, acc: 0.082921]\n",
      "1392  [AE loss: 0.286020, acc: 0.082272]\n",
      "1393  [AE loss: 0.285021, acc: 0.082946]\n",
      "1394  [AE loss: 0.282907, acc: 0.083552]\n",
      "1395  [AE loss: 0.285859, acc: 0.082859]\n",
      "1396  [AE loss: 0.288095, acc: 0.083408]\n",
      "1397  [AE loss: 0.289621, acc: 0.083433]\n",
      "1398  [AE loss: 0.293042, acc: 0.083318]\n",
      "1399  [AE loss: 0.290665, acc: 0.082588]\n",
      "1400  [AE loss: 0.294532, acc: 0.082461]\n",
      "1401  [AE loss: 0.286369, acc: 0.082208]\n",
      "1402  [AE loss: 0.287360, acc: 0.083245]\n",
      "1403  [AE loss: 0.287304, acc: 0.083080]\n",
      "1404  [AE loss: 0.283138, acc: 0.082482]\n",
      "1405  [AE loss: 0.283719, acc: 0.082769]\n",
      "1406  [AE loss: 0.285890, acc: 0.082893]\n",
      "1407  [AE loss: 0.290852, acc: 0.083115]\n",
      "1408  [AE loss: 0.285547, acc: 0.082472]\n",
      "1409  [AE loss: 0.288687, acc: 0.082100]\n",
      "1410  [AE loss: 0.286475, acc: 0.083118]\n",
      "1411  [AE loss: 0.285763, acc: 0.082463]\n",
      "1412  [AE loss: 0.294390, acc: 0.083003]\n",
      "1413  [AE loss: 0.294412, acc: 0.083434]\n",
      "1414  [AE loss: 0.309184, acc: 0.083062]\n",
      "1415  [AE loss: 0.286223, acc: 0.082164]\n",
      "1416  [AE loss: 0.287508, acc: 0.082756]\n",
      "1417  [AE loss: 0.285845, acc: 0.084187]\n",
      "1418  [AE loss: 0.286577, acc: 0.083954]\n",
      "1419  [AE loss: 0.285564, acc: 0.083488]\n",
      "1420  [AE loss: 0.292681, acc: 0.081779]\n",
      "1421  [AE loss: 0.290816, acc: 0.083777]\n",
      "1422  [AE loss: 0.290232, acc: 0.082755]\n",
      "1423  [AE loss: 0.286538, acc: 0.082917]\n",
      "1424  [AE loss: 0.288574, acc: 0.083746]\n",
      "1425  [AE loss: 0.288572, acc: 0.082859]\n",
      "1426  [AE loss: 0.293431, acc: 0.082155]\n",
      "1427  [AE loss: 0.299304, acc: 0.082546]\n",
      "1428  [AE loss: 0.317539, acc: 0.083097]\n",
      "1429  [AE loss: 0.285183, acc: 0.083599]\n",
      "1430  [AE loss: 0.286102, acc: 0.083243]\n",
      "1431  [AE loss: 0.288194, acc: 0.082649]\n",
      "1432  [AE loss: 0.284354, acc: 0.083124]\n",
      "1433  [AE loss: 0.285447, acc: 0.082968]\n",
      "1434  [AE loss: 0.284282, acc: 0.082825]\n",
      "1435  [AE loss: 0.285695, acc: 0.083065]\n",
      "1436  [AE loss: 0.288060, acc: 0.082366]\n",
      "1437  [AE loss: 0.288254, acc: 0.082837]\n",
      "1438  [AE loss: 0.287106, acc: 0.082493]\n",
      "1439  [AE loss: 0.285025, acc: 0.083179]\n",
      "1440  [AE loss: 0.289369, acc: 0.082455]\n",
      "1441  [AE loss: 0.292613, acc: 0.083092]\n",
      "1442  [AE loss: 0.302389, acc: 0.082458]\n",
      "1443  [AE loss: 0.289980, acc: 0.084193]\n",
      "1444  [AE loss: 0.284680, acc: 0.083636]\n",
      "1445  [AE loss: 0.286565, acc: 0.083429]\n",
      "1446  [AE loss: 0.286250, acc: 0.082314]\n",
      "1447  [AE loss: 0.286090, acc: 0.082230]\n",
      "1448  [AE loss: 0.286406, acc: 0.081984]\n",
      "1449  [AE loss: 0.285916, acc: 0.082766]\n",
      "1450  [AE loss: 0.291648, acc: 0.082284]\n",
      "1451  [AE loss: 0.309798, acc: 0.083035]\n",
      "1452  [AE loss: 0.316761, acc: 0.083246]\n",
      "1453  [AE loss: 0.295374, acc: 0.082720]\n",
      "1454  [AE loss: 0.296348, acc: 0.082643]\n",
      "1455  [AE loss: 0.285206, acc: 0.082950]\n",
      "1456  [AE loss: 0.289250, acc: 0.082703]\n",
      "1457  [AE loss: 0.287313, acc: 0.083080]\n",
      "1458  [AE loss: 0.287290, acc: 0.082959]\n",
      "1459  [AE loss: 0.284130, acc: 0.082856]\n",
      "1460  [AE loss: 0.288054, acc: 0.082505]\n",
      "1461  [AE loss: 0.285659, acc: 0.082825]\n",
      "1462  [AE loss: 0.286940, acc: 0.083131]\n",
      "1463  [AE loss: 0.285177, acc: 0.083569]\n",
      "1464  [AE loss: 0.286066, acc: 0.082924]\n",
      "1465  [AE loss: 0.287213, acc: 0.082629]\n",
      "1466  [AE loss: 0.285967, acc: 0.082635]\n",
      "1467  [AE loss: 0.288419, acc: 0.082585]\n",
      "1468  [AE loss: 0.287634, acc: 0.083062]\n",
      "1469  [AE loss: 0.284540, acc: 0.083163]\n",
      "1470  [AE loss: 0.286379, acc: 0.082720]\n",
      "1471  [AE loss: 0.289073, acc: 0.082616]\n",
      "1472  [AE loss: 0.300484, acc: 0.083254]\n",
      "1473  [AE loss: 0.304285, acc: 0.082999]\n",
      "1474  [AE loss: 0.290037, acc: 0.083027]\n",
      "1475  [AE loss: 0.286385, acc: 0.082932]\n",
      "1476  [AE loss: 0.285570, acc: 0.083447]\n",
      "1477  [AE loss: 0.287466, acc: 0.082136]\n",
      "1478  [AE loss: 0.285580, acc: 0.083127]\n",
      "1479  [AE loss: 0.285376, acc: 0.082352]\n",
      "1480  [AE loss: 0.287101, acc: 0.083329]\n",
      "1481  [AE loss: 0.285600, acc: 0.082839]\n",
      "1482  [AE loss: 0.288420, acc: 0.083403]\n",
      "1483  [AE loss: 0.290092, acc: 0.082587]\n",
      "1484  [AE loss: 0.301507, acc: 0.082896]\n",
      "1485  [AE loss: 0.298419, acc: 0.082858]\n",
      "1486  [AE loss: 0.299607, acc: 0.082692]\n",
      "1487  [AE loss: 0.291680, acc: 0.082510]\n",
      "1488  [AE loss: 0.295310, acc: 0.082537]\n",
      "1489  [AE loss: 0.287048, acc: 0.083639]\n",
      "1490  [AE loss: 0.286315, acc: 0.082738]\n",
      "1491  [AE loss: 0.286324, acc: 0.082632]\n",
      "1492  [AE loss: 0.284855, acc: 0.082706]\n",
      "1493  [AE loss: 0.285483, acc: 0.082384]\n",
      "1494  [AE loss: 0.287529, acc: 0.083137]\n",
      "1495  [AE loss: 0.283588, acc: 0.082219]\n",
      "1496  [AE loss: 0.283264, acc: 0.083409]\n",
      "1497  [AE loss: 0.284795, acc: 0.082831]\n",
      "1498  [AE loss: 0.286306, acc: 0.083654]\n",
      "1499  [AE loss: 0.289777, acc: 0.083390]\n",
      "1500  [AE loss: 0.298134, acc: 0.082769]\n",
      "1501  [AE loss: 0.287974, acc: 0.083127]\n",
      "1502  [AE loss: 0.296407, acc: 0.083683]\n",
      "1503  [AE loss: 0.291503, acc: 0.083436]\n",
      "1504  [AE loss: 0.294361, acc: 0.082842]\n",
      "1505  [AE loss: 0.288175, acc: 0.083173]\n",
      "1506  [AE loss: 0.304172, acc: 0.083296]\n",
      "1507  [AE loss: 0.289103, acc: 0.083335]\n",
      "1508  [AE loss: 0.287629, acc: 0.082859]\n",
      "1509  [AE loss: 0.287048, acc: 0.082876]\n",
      "1510  [AE loss: 0.288184, acc: 0.083113]\n",
      "1511  [AE loss: 0.283993, acc: 0.082804]\n",
      "1512  [AE loss: 0.286593, acc: 0.082950]\n",
      "1513  [AE loss: 0.283500, acc: 0.083116]\n",
      "1514  [AE loss: 0.284908, acc: 0.083397]\n",
      "1515  [AE loss: 0.284445, acc: 0.082146]\n",
      "1516  [AE loss: 0.288885, acc: 0.082588]\n",
      "1517  [AE loss: 0.290107, acc: 0.082654]\n",
      "1518  [AE loss: 0.288160, acc: 0.082938]\n",
      "1519  [AE loss: 0.289144, acc: 0.083541]\n",
      "1520  [AE loss: 0.289118, acc: 0.083329]\n",
      "1521  [AE loss: 0.286883, acc: 0.083016]\n",
      "1522  [AE loss: 0.286130, acc: 0.082739]\n",
      "1523  [AE loss: 0.283711, acc: 0.083136]\n",
      "1524  [AE loss: 0.285411, acc: 0.082007]\n",
      "1525  [AE loss: 0.291113, acc: 0.083092]\n",
      "1526  [AE loss: 0.298827, acc: 0.083110]\n",
      "1527  [AE loss: 0.293148, acc: 0.082870]\n",
      "1528  [AE loss: 0.295060, acc: 0.083247]\n",
      "1529  [AE loss: 0.286172, acc: 0.083243]\n",
      "1530  [AE loss: 0.284168, acc: 0.082770]\n",
      "1531  [AE loss: 0.285291, acc: 0.083004]\n",
      "1532  [AE loss: 0.289071, acc: 0.083190]\n",
      "1533  [AE loss: 0.287103, acc: 0.083326]\n",
      "1534  [AE loss: 0.287084, acc: 0.082823]\n",
      "1535  [AE loss: 0.291009, acc: 0.082347]\n",
      "1536  [AE loss: 0.291265, acc: 0.083212]\n",
      "1537  [AE loss: 0.292159, acc: 0.082894]\n",
      "1538  [AE loss: 0.291578, acc: 0.083074]\n",
      "1539  [AE loss: 0.284024, acc: 0.083099]\n",
      "1540  [AE loss: 0.285789, acc: 0.082816]\n",
      "1541  [AE loss: 0.285784, acc: 0.082480]\n",
      "1542  [AE loss: 0.283395, acc: 0.083030]\n",
      "1543  [AE loss: 0.283942, acc: 0.083441]\n",
      "1544  [AE loss: 0.288302, acc: 0.082625]\n",
      "1545  [AE loss: 0.292450, acc: 0.082977]\n",
      "1546  [AE loss: 0.300882, acc: 0.082375]\n",
      "1547  [AE loss: 0.293625, acc: 0.082290]\n",
      "1548  [AE loss: 0.366397, acc: 0.082910]\n",
      "1549  [AE loss: 0.299728, acc: 0.083083]\n",
      "1550  [AE loss: 0.292784, acc: 0.083635]\n",
      "1551  [AE loss: 0.292038, acc: 0.082653]\n",
      "1552  [AE loss: 0.290407, acc: 0.082615]\n",
      "1553  [AE loss: 0.286853, acc: 0.082805]\n",
      "1554  [AE loss: 0.288155, acc: 0.083458]\n",
      "1555  [AE loss: 0.284454, acc: 0.082615]\n",
      "1556  [AE loss: 0.287405, acc: 0.082678]\n",
      "1557  [AE loss: 0.285976, acc: 0.082494]\n",
      "1558  [AE loss: 0.284251, acc: 0.082411]\n",
      "1559  [AE loss: 0.287659, acc: 0.083049]\n",
      "1560  [AE loss: 0.290255, acc: 0.082942]\n",
      "1561  [AE loss: 0.296952, acc: 0.082540]\n",
      "1562  [AE loss: 0.285199, acc: 0.083202]\n",
      "1563  [AE loss: 0.285038, acc: 0.083453]\n",
      "1564  [AE loss: 0.286027, acc: 0.083715]\n",
      "1565  [AE loss: 0.283942, acc: 0.082947]\n",
      "1566  [AE loss: 0.283473, acc: 0.083572]\n",
      "1567  [AE loss: 0.287744, acc: 0.083328]\n",
      "1568  [AE loss: 0.286709, acc: 0.083192]\n",
      "1569  [AE loss: 0.283271, acc: 0.083429]\n",
      "1570  [AE loss: 0.286598, acc: 0.083031]\n",
      "1571  [AE loss: 0.287391, acc: 0.083052]\n",
      "1572  [AE loss: 0.283710, acc: 0.083719]\n",
      "1573  [AE loss: 0.285926, acc: 0.083545]\n",
      "1574  [AE loss: 0.285225, acc: 0.083048]\n",
      "1575  [AE loss: 0.296208, acc: 0.082173]\n",
      "1576  [AE loss: 0.287917, acc: 0.083037]\n",
      "1577  [AE loss: 0.291150, acc: 0.082424]\n",
      "1578  [AE loss: 0.287771, acc: 0.082692]\n",
      "1579  [AE loss: 0.285623, acc: 0.082968]\n",
      "1580  [AE loss: 0.286972, acc: 0.082701]\n",
      "1581  [AE loss: 0.286401, acc: 0.081740]\n",
      "1582  [AE loss: 0.286494, acc: 0.082666]\n",
      "1583  [AE loss: 0.295197, acc: 0.083004]\n",
      "1584  [AE loss: 0.287420, acc: 0.082526]\n",
      "1585  [AE loss: 0.292918, acc: 0.083348]\n",
      "1586  [AE loss: 0.286952, acc: 0.082653]\n",
      "1587  [AE loss: 0.288184, acc: 0.083621]\n",
      "1588  [AE loss: 0.284425, acc: 0.082788]\n",
      "1589  [AE loss: 0.288342, acc: 0.083042]\n",
      "1590  [AE loss: 0.286355, acc: 0.083091]\n",
      "1591  [AE loss: 0.285564, acc: 0.083914]\n",
      "1592  [AE loss: 0.284159, acc: 0.082881]\n",
      "1593  [AE loss: 0.283827, acc: 0.082980]\n",
      "1594  [AE loss: 0.285069, acc: 0.082441]\n",
      "1595  [AE loss: 0.283614, acc: 0.082800]\n",
      "1596  [AE loss: 0.285204, acc: 0.081711]\n",
      "1597  [AE loss: 0.289758, acc: 0.082861]\n",
      "1598  [AE loss: 0.293694, acc: 0.082792]\n",
      "1599  [AE loss: 0.306341, acc: 0.082844]\n",
      "1600  [AE loss: 0.285730, acc: 0.083143]\n",
      "1601  [AE loss: 0.285053, acc: 0.082969]\n",
      "1602  [AE loss: 0.286763, acc: 0.083535]\n",
      "1603  [AE loss: 0.284869, acc: 0.082620]\n",
      "1604  [AE loss: 0.283882, acc: 0.083643]\n",
      "1605  [AE loss: 0.287468, acc: 0.082440]\n",
      "1606  [AE loss: 0.289075, acc: 0.082963]\n",
      "1607  [AE loss: 0.291485, acc: 0.082960]\n",
      "1608  [AE loss: 0.291985, acc: 0.083262]\n",
      "1609  [AE loss: 0.287719, acc: 0.083522]\n",
      "1610  [AE loss: 0.286248, acc: 0.082767]\n",
      "1611  [AE loss: 0.287248, acc: 0.082004]\n",
      "1612  [AE loss: 0.286216, acc: 0.083074]\n",
      "1613  [AE loss: 0.284498, acc: 0.083898]\n",
      "1614  [AE loss: 0.286055, acc: 0.083850]\n",
      "1615  [AE loss: 0.286768, acc: 0.083186]\n",
      "1616  [AE loss: 0.287456, acc: 0.083134]\n",
      "1617  [AE loss: 0.293611, acc: 0.082263]\n",
      "1618  [AE loss: 0.299740, acc: 0.082815]\n",
      "1619  [AE loss: 0.285159, acc: 0.083177]\n",
      "1620  [AE loss: 0.289322, acc: 0.083539]\n",
      "1621  [AE loss: 0.284817, acc: 0.083187]\n",
      "1622  [AE loss: 0.286795, acc: 0.082892]\n",
      "1623  [AE loss: 0.287967, acc: 0.082438]\n",
      "1624  [AE loss: 0.287742, acc: 0.082856]\n",
      "1625  [AE loss: 0.288856, acc: 0.083350]\n",
      "1626  [AE loss: 0.289588, acc: 0.083444]\n",
      "1627  [AE loss: 0.287186, acc: 0.083608]\n",
      "1628  [AE loss: 0.284689, acc: 0.083614]\n",
      "1629  [AE loss: 0.286588, acc: 0.083042]\n",
      "1630  [AE loss: 0.286098, acc: 0.082562]\n",
      "1631  [AE loss: 0.289803, acc: 0.083250]\n",
      "1632  [AE loss: 0.311637, acc: 0.082859]\n",
      "1633  [AE loss: 0.308683, acc: 0.082538]\n",
      "1634  [AE loss: 0.302515, acc: 0.083229]\n",
      "1635  [AE loss: 0.290423, acc: 0.082625]\n",
      "1636  [AE loss: 0.288465, acc: 0.082802]\n",
      "1637  [AE loss: 0.286250, acc: 0.083323]\n",
      "1638  [AE loss: 0.287825, acc: 0.083528]\n",
      "1639  [AE loss: 0.288385, acc: 0.082805]\n",
      "1640  [AE loss: 0.283193, acc: 0.082480]\n",
      "1641  [AE loss: 0.287559, acc: 0.082533]\n",
      "1642  [AE loss: 0.286901, acc: 0.083418]\n",
      "1643  [AE loss: 0.285907, acc: 0.082339]\n",
      "1644  [AE loss: 0.283160, acc: 0.082600]\n",
      "1645  [AE loss: 0.288757, acc: 0.082767]\n",
      "1646  [AE loss: 0.286506, acc: 0.082864]\n",
      "1647  [AE loss: 0.300405, acc: 0.082170]\n",
      "1648  [AE loss: 0.288478, acc: 0.082756]\n",
      "1649  [AE loss: 0.287945, acc: 0.083004]\n",
      "1650  [AE loss: 0.281890, acc: 0.083141]\n",
      "1651  [AE loss: 0.285327, acc: 0.082935]\n",
      "1652  [AE loss: 0.283720, acc: 0.082662]\n",
      "1653  [AE loss: 0.285784, acc: 0.082673]\n",
      "1654  [AE loss: 0.286802, acc: 0.082216]\n",
      "1655  [AE loss: 0.284685, acc: 0.082765]\n",
      "1656  [AE loss: 0.282836, acc: 0.083752]\n",
      "1657  [AE loss: 0.285970, acc: 0.083344]\n",
      "1658  [AE loss: 0.284301, acc: 0.082963]\n",
      "1659  [AE loss: 0.284400, acc: 0.082034]\n",
      "1660  [AE loss: 0.289365, acc: 0.083116]\n",
      "1661  [AE loss: 0.284731, acc: 0.082240]\n",
      "1662  [AE loss: 0.300282, acc: 0.083262]\n",
      "1663  [AE loss: 0.304641, acc: 0.082621]\n",
      "1664  [AE loss: 0.288574, acc: 0.082347]\n",
      "1665  [AE loss: 0.286243, acc: 0.083181]\n",
      "1666  [AE loss: 0.286166, acc: 0.083165]\n",
      "1667  [AE loss: 0.284307, acc: 0.082552]\n",
      "1668  [AE loss: 0.287070, acc: 0.082316]\n",
      "1669  [AE loss: 0.287939, acc: 0.082285]\n",
      "1670  [AE loss: 0.286389, acc: 0.083096]\n",
      "1671  [AE loss: 0.286099, acc: 0.082018]\n",
      "1672  [AE loss: 0.286369, acc: 0.082756]\n",
      "1673  [AE loss: 0.286710, acc: 0.082813]\n",
      "1674  [AE loss: 0.286913, acc: 0.082797]\n",
      "1675  [AE loss: 0.291440, acc: 0.083358]\n",
      "1676  [AE loss: 0.299270, acc: 0.082480]\n",
      "1677  [AE loss: 0.293048, acc: 0.083236]\n",
      "1678  [AE loss: 0.291625, acc: 0.083492]\n",
      "1679  [AE loss: 0.286065, acc: 0.084259]\n",
      "1680  [AE loss: 0.285906, acc: 0.083503]\n",
      "1681  [AE loss: 0.288002, acc: 0.083098]\n",
      "1682  [AE loss: 0.285457, acc: 0.082728]\n",
      "1683  [AE loss: 0.284198, acc: 0.082948]\n",
      "1684  [AE loss: 0.286043, acc: 0.082745]\n",
      "1685  [AE loss: 0.287344, acc: 0.082953]\n",
      "1686  [AE loss: 0.292774, acc: 0.082295]\n",
      "1687  [AE loss: 0.295946, acc: 0.083228]\n",
      "1688  [AE loss: 0.285327, acc: 0.082904]\n",
      "1689  [AE loss: 0.289171, acc: 0.082294]\n",
      "1690  [AE loss: 0.284538, acc: 0.083027]\n",
      "1691  [AE loss: 0.286369, acc: 0.082551]\n",
      "1692  [AE loss: 0.286194, acc: 0.082260]\n",
      "1693  [AE loss: 0.286273, acc: 0.083057]\n",
      "1694  [AE loss: 0.286660, acc: 0.082625]\n",
      "1695  [AE loss: 0.287129, acc: 0.082549]\n",
      "1696  [AE loss: 0.284740, acc: 0.083381]\n",
      "1697  [AE loss: 0.289727, acc: 0.082466]\n",
      "1698  [AE loss: 0.293499, acc: 0.083156]\n",
      "1699  [AE loss: 0.289700, acc: 0.082538]\n",
      "1700  [AE loss: 0.292226, acc: 0.082540]\n",
      "1701  [AE loss: 0.285514, acc: 0.083158]\n",
      "1702  [AE loss: 0.286398, acc: 0.082555]\n",
      "1703  [AE loss: 0.287581, acc: 0.083414]\n",
      "1704  [AE loss: 0.289078, acc: 0.082209]\n",
      "1705  [AE loss: 0.289554, acc: 0.082268]\n",
      "1706  [AE loss: 0.289904, acc: 0.083293]\n",
      "1707  [AE loss: 0.292010, acc: 0.082548]\n",
      "1708  [AE loss: 0.288856, acc: 0.083341]\n",
      "1709  [AE loss: 0.290144, acc: 0.083151]\n",
      "1710  [AE loss: 0.289536, acc: 0.083199]\n",
      "1711  [AE loss: 0.293880, acc: 0.082493]\n",
      "1712  [AE loss: 0.293670, acc: 0.083101]\n",
      "1713  [AE loss: 0.287916, acc: 0.082506]\n",
      "1714  [AE loss: 0.287446, acc: 0.082604]\n",
      "1715  [AE loss: 0.290479, acc: 0.083230]\n",
      "1716  [AE loss: 0.288362, acc: 0.082445]\n",
      "1717  [AE loss: 0.288384, acc: 0.083096]\n",
      "1718  [AE loss: 0.285971, acc: 0.082708]\n",
      "1719  [AE loss: 0.284512, acc: 0.083550]\n",
      "1720  [AE loss: 0.284836, acc: 0.082810]\n",
      "1721  [AE loss: 0.286836, acc: 0.082946]\n",
      "1722  [AE loss: 0.282622, acc: 0.082883]\n",
      "1723  [AE loss: 0.288214, acc: 0.082787]\n",
      "1724  [AE loss: 0.291361, acc: 0.083412]\n",
      "1725  [AE loss: 0.290670, acc: 0.083635]\n",
      "1726  [AE loss: 0.292123, acc: 0.082695]\n",
      "1727  [AE loss: 0.288063, acc: 0.081897]\n",
      "1728  [AE loss: 0.287140, acc: 0.082321]\n",
      "1729  [AE loss: 0.289666, acc: 0.082932]\n",
      "1730  [AE loss: 0.287519, acc: 0.082804]\n",
      "1731  [AE loss: 0.284529, acc: 0.083115]\n",
      "1732  [AE loss: 0.285842, acc: 0.083450]\n",
      "1733  [AE loss: 0.285487, acc: 0.083491]\n",
      "1734  [AE loss: 0.287718, acc: 0.083246]\n",
      "1735  [AE loss: 0.283855, acc: 0.082170]\n",
      "1736  [AE loss: 0.285102, acc: 0.082640]\n",
      "1737  [AE loss: 0.288597, acc: 0.082062]\n",
      "1738  [AE loss: 0.294715, acc: 0.082897]\n",
      "1739  [AE loss: 0.293575, acc: 0.082057]\n",
      "1740  [AE loss: 0.294175, acc: 0.082383]\n",
      "1741  [AE loss: 0.287231, acc: 0.082611]\n",
      "1742  [AE loss: 0.288428, acc: 0.083149]\n",
      "1743  [AE loss: 0.283588, acc: 0.083115]\n",
      "1744  [AE loss: 0.285735, acc: 0.083381]\n",
      "1745  [AE loss: 0.285046, acc: 0.083036]\n",
      "1746  [AE loss: 0.287750, acc: 0.082653]\n",
      "1747  [AE loss: 0.285674, acc: 0.082588]\n",
      "1748  [AE loss: 0.284497, acc: 0.083024]\n",
      "1749  [AE loss: 0.286868, acc: 0.083096]\n",
      "1750  [AE loss: 0.289804, acc: 0.082710]\n",
      "1751  [AE loss: 0.297655, acc: 0.082992]\n",
      "1752  [AE loss: 0.290267, acc: 0.082194]\n",
      "1753  [AE loss: 0.291921, acc: 0.083206]\n",
      "1754  [AE loss: 0.285367, acc: 0.082356]\n",
      "1755  [AE loss: 0.285749, acc: 0.082610]\n",
      "1756  [AE loss: 0.284715, acc: 0.082687]\n",
      "1757  [AE loss: 0.285321, acc: 0.082092]\n",
      "1758  [AE loss: 0.283448, acc: 0.083108]\n",
      "1759  [AE loss: 0.284074, acc: 0.083412]\n",
      "1760  [AE loss: 0.286054, acc: 0.082378]\n",
      "1761  [AE loss: 0.286557, acc: 0.082676]\n",
      "1762  [AE loss: 0.286571, acc: 0.082692]\n",
      "1763  [AE loss: 0.293373, acc: 0.082399]\n",
      "1764  [AE loss: 0.299455, acc: 0.083055]\n",
      "1765  [AE loss: 0.289493, acc: 0.082203]\n",
      "1766  [AE loss: 0.301446, acc: 0.082893]\n",
      "1767  [AE loss: 0.287078, acc: 0.082405]\n",
      "1768  [AE loss: 0.286998, acc: 0.082064]\n",
      "1769  [AE loss: 0.289262, acc: 0.083325]\n",
      "1770  [AE loss: 0.284841, acc: 0.083350]\n",
      "1771  [AE loss: 0.287019, acc: 0.082798]\n",
      "1772  [AE loss: 0.286351, acc: 0.082148]\n",
      "1773  [AE loss: 0.285783, acc: 0.082469]\n",
      "1774  [AE loss: 0.289467, acc: 0.083326]\n",
      "1775  [AE loss: 0.287003, acc: 0.082579]\n",
      "1776  [AE loss: 0.287136, acc: 0.082806]\n",
      "1777  [AE loss: 0.286809, acc: 0.083004]\n",
      "1778  [AE loss: 0.292229, acc: 0.083059]\n",
      "1779  [AE loss: 0.284741, acc: 0.083429]\n",
      "1780  [AE loss: 0.287666, acc: 0.083831]\n",
      "1781  [AE loss: 0.282515, acc: 0.082574]\n",
      "1782  [AE loss: 0.286044, acc: 0.082384]\n",
      "1783  [AE loss: 0.283546, acc: 0.082747]\n",
      "1784  [AE loss: 0.284587, acc: 0.083029]\n",
      "1785  [AE loss: 0.286783, acc: 0.082867]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-9b88f2ef7183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfksims_ae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFKSIMS_AE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtimer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElapsedTimer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfksims_ae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfksims_ae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-127553fda2fb>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_steps, batch_size, save_interval)\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mtmix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_mixed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mtfgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_fgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mae_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mlog_mesg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%d  [AE loss: %f, acc: %f]\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mae_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mae_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_mesg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\My_Pythonanaconda3\\envs\\Neural_Net_env\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, class_weight, sample_weight)\u001b[0m\n\u001b[0;32m   1065\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[0;32m   1066\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m                                          class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[1;32mD:\\My_Pythonanaconda3\\envs\\Neural_Net_env\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1888\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1890\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1891\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1892\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\My_Pythonanaconda3\\envs\\Neural_Net_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\My_Pythonanaconda3\\envs\\Neural_Net_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\My_Pythonanaconda3\\envs\\Neural_Net_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\My_Pythonanaconda3\\envs\\Neural_Net_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\My_Pythonanaconda3\\envs\\Neural_Net_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\My_Pythonanaconda3\\envs\\Neural_Net_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fksims_ae = FKSIMS_AE()\n",
    "timer = ElapsedTimer()\n",
    "fksims_ae.train(train_steps=10000, batch_size=200, save_interval=50)\n",
    "timer.elapsed_time()\n",
    "fksims_ae.plot_images(fake=True)\n",
    "fksims_ae.plot_images(fake=False, save2file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
